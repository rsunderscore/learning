{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bC7-g3yGJCdW"
   },
   "source": [
    "# Fashion-MNIST Neural Network\n",
    "\n",
    "This notebook implements a simple NN consisting of two dense layers, and uses this network to classify Fashion-MNIST images.\n",
    "\n",
    "These Fashion-MNIST images consist of clothing items from these ten classes. (*T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)*\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "built into keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tL0ZtKVL6NN4"
   },
   "source": [
    "## Load correct version of TensorFlow\n",
    "\n",
    "Before we use TensorFlow we must load the correct version. We want version 2.x. To do this we execute the Colab commands below. Note that these commands ONLY WORK IN COLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leSo_zfF6QXV"
   },
   "outputs": [],
   "source": [
    "# Install TensorFlow using Colab's tensorflow_version command\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sy_y_E9_NFc8"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "We import TensorFlow, Numpy, and Matplotlib libraries.  \n",
    "\n",
    "Numpy is a powerful n-dimensional array library that\n",
    "allows us to easily create and manipulate arrays of data, and more!\n",
    "\n",
    "Numpy also allows us to convert TensorFlow's native data structures,\n",
    "to Python native data types.\n",
    "\n",
    "Matplotlib is a graphics plot library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgDK_bJeJpua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMWdw_E_QPhY"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Using example data, train a model that will correctly predict the class of images of fashion items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N49Xfo21RTr9"
   },
   "source": [
    "## Get Data\n",
    "\n",
    "### About the Fashion-MNIST dataset\n",
    "\n",
    "The Fashion-MNIST dataset is a widely available dataset.  It is documented at the https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Each image in the dataset is 28 X 28 pixels.  The pixel values are a grayscale with values ranging from 0 to 255.\n",
    "\n",
    "Associated with each image is a number from 0 to 9 specifying the class of the fashion item in the image.  The values for the classes are:\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "|   1    | Trouser    | \n",
    "|  2     |  Pullover  |\n",
    "|   3    |    Dress    |\n",
    "|    4   |    Coat      |\n",
    "|   5    | Sandal     |\n",
    "|   6    | Shirt         |\n",
    "|    7   |  Sneaker  |\n",
    "|   8    |  Bag          |\n",
    "|  9     | Ankle boot |\n",
    "\n",
    "There are 60,000 Training examples and 10,000 Testing examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qswhm4c7nMVl"
   },
   "source": [
    "We will need these class names later in the code.  So we create an list of names we can index by the class label number.  For example, class_names[2] = 'Pullover' and class_names[9] = 'Ankle boot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK737-ABm7od"
   },
   "outputs": [],
   "source": [
    "# Define class names to display\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcAJeHCCm_TW"
   },
   "source": [
    "Fashion-MNIST is one of the datasets provided with Keras in Tensorflow.  Keras' Dataset library provides a load_data() method that will download the dataset of 60,000 Training images and their corresponding 60,000 labels, and 10,000 Test images and their corresponding 10,000 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boston_housing',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'fashion_mnist',\n",
       " 'imdb',\n",
       " 'mnist',\n",
       " 'reuters']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(tf.keras.datasets) if '_' != x[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MMs613GNLe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAcxPIjJkLKZ"
   },
   "source": [
    "### Explore the data\n",
    "\n",
    "Let's look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Umx96CdYV-yz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 28, 28) (60000,)\n",
      "Test data: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the data tensors we loaded.  Should be:\n",
    "#   60,000 training 28X28 images and their labels, and 10,000 testing images and their labels\n",
    "print ('Training data:', train_images.shape, train_labels.shape)\n",
    "print ('Test data:', test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Jq_XBs9Xa6C"
   },
   "source": [
    "### Inspect data\n",
    "Each image should be a 28 X 28 image and have a gray scale value of 0-255. Let's display an arbitrary image.\n",
    "\n",
    "You should see:  \n",
    "* A title that specifies the class of the image, whose description is shown on the table above.\n",
    "* The image \n",
    "* The 28 by 28 width and height pixel index \n",
    "* and on the right the grayscale legend associated with the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ni0bRFhjWP1y"
   },
   "outputs": [],
   "source": [
    "def show_training_image(index):\n",
    "  img_label = str(train_labels[index]) + ' (' +  class_names[train_labels[index]] + ')'\n",
    "  plt.figure()\n",
    "  plt.title('Image Label ' + img_label) \n",
    "  plt.imshow(train_images[index], cmap='gray')  # data is grayscale, but displays in color without cmap='gray'\n",
    "  plt.colorbar()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJa1Wq_4acop"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_index = 100\n",
    "show_training_image(img_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db_TW6wccWHw"
   },
   "source": [
    "## Prepare Data\n",
    "\n",
    "As usual we need to do some pre-processing of the data.  Here we want to scale the pixels values from 0 to 255 to 0.0 to 1.0.  We scale both the training and testing image values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0vDE9tBYUWx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scale training and testing image values\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Print the image again and notice the values now range from 0 to 1.\n",
    "#   And the image looks the same, just on a different scale. \n",
    "show_training_image(img_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAAj8oXkeBsY"
   },
   "source": [
    "## Create Model\n",
    "\n",
    "We can now create the model we are going to train with the data.\n",
    "\n",
    "This will be a simple model that:\n",
    "* Flattens the 28 X 28 pixel values into a long stream of 28 by 28 = 784 pixel values - Note this is a Keras layer but is not a neural network layer. Notice the input_shape parameter contains only the shape of a single data element.  There are n data elements each with this same shape. \n",
    "* Passes each pixel value as the input to each of the 128 neurons.  \n",
    "* And finally passes values to the last layer that contains 10 neurons, one for each of the ten classes (t-shirt, pullover, etc).  Each of these 10 neurons uses the Softmax activation function to determine the probability that the image is each class.  The final output from the model is a vector of probabilities that the image is of each class.  Such as :\n",
    "[0.01, 0.05, 0.04, 0.06, 0.50, 0.20, 0.04, 0.00, 0.03, 0.07].  This example vector adds up to 1.0 and shows the probability that the image is class 0 (T-shirt/top) is 0.01 (1%), class 1 (Trousers) is 0.05 (5%), class 2 (Pullover) is 0.04 (4%), ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0N2R893MdSZn"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()      # Create a new sequential model\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))    # keras processing layer - no neurons\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', name='dense-128-relu'))   # 128 neurons connected to pixels\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='dense-10-softmax')) # determines probability of each of the 10 classes\n",
    "#using softmax to calc probabilities for diff  classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nu2185LTBq8"
   },
   "source": [
    "### Structure of the model\n",
    "\n",
    "To make it easy for us to visualize the model, Keras models have a summary method.  When we call it we see our model structure.  \n",
    "\n",
    "For completeness we also show the shape of the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gXXuAqQURdH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (60000, 28, 28)\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense-128-relu (Dense)       (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense-10-softmax (Dense)     (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Input Shape:', train_images.shape)\n",
    "print()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJTmzxCJ3Bpi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Before we can train the model we need to specify how the model will learn from the training data.  To do this we specify:\n",
    "*  loss - how we measure loss (error).  We will use sparse_categorical_crossentropy, which determines the highest predicted class and calculates loss based on how often this is the correct class.\n",
    "*     optimizer - how the model will update the model's weights to reduce the loss.  We use the Adam variant of Mini-batch Gradient Descent.\n",
    "* metrics  - the metrics used for evaluation of training and test.  In this case we use accuracy, that is how often the images are correctly classified.  Higher is better. (1.00 would be perfect (100%), 0,75 = 75%, 0.25 = 25%, ...) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW9pZ8kw2Nid"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7wyz8jh6ZA6"
   },
   "source": [
    "## Train the Model\n",
    "Now that we have our data and model, and have specified how the model will learn from the data we can train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osxdZq5y6qF1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.4967 - accuracy: 0.8255\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.3736 - accuracy: 0.8650\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3335 - accuracy: 0.8779\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3137 - accuracy: 0.8845\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2935 - accuracy: 0.8913\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2787 - accuracy: 0.8963\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2679 - accuracy: 0.9007\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2561 - accuracy: 0.9054\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2469 - accuracy: 0.9087\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2394 - accuracy: 0.9099\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2307 - accuracy: 0.9147\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2230 - accuracy: 0.9166\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2151 - accuracy: 0.9182\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2097 - accuracy: 0.9202\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2046 - accuracy: 0.9227\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1974 - accuracy: 0.9261\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1947 - accuracy: 0.9264\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1893 - accuracy: 0.9290\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1829 - accuracy: 0.9310\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1787 - accuracy: 0.9331\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1739 - accuracy: 0.9347\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1707 - accuracy: 0.9357\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1649 - accuracy: 0.9380\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1634 - accuracy: 0.9376\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1594 - accuracy: 0.9398\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1555 - accuracy: 0.9409\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1528 - accuracy: 0.9418\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1476 - accuracy: 0.9440\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1440 - accuracy: 0.9448\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1405 - accuracy: 0.9469\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1381 - accuracy: 0.9482\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1368 - accuracy: 0.9480\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1331 - accuracy: 0.9491\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1301 - accuracy: 0.9502\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1289 - accuracy: 0.9513\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1251 - accuracy: 0.9523\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1240 - accuracy: 0.9529\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1221 - accuracy: 0.9542\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1190 - accuracy: 0.9552\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1164 - accuracy: 0.9557\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#90 seconds\n",
    "train_hist = model.fit(train_images, train_labels, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2TLdEfR9tus"
   },
   "source": [
    "Plot the training to makes sure it is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hCztJ2o90DS"
   },
   "outputs": [],
   "source": [
    "def plot_acc(hist):\n",
    "  # plot the accuracy\n",
    "  plt.title('Accuracy History')\n",
    "  plt.plot(hist.history['accuracy'])\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()\n",
    "  \n",
    "def plot_loss(hist):\n",
    "  # plot the loss\n",
    "  plt.title('Loss History')\n",
    "  plt.plot(hist.history['loss'])\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zljp5aHqv514"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(train_hist)\n",
    "plot_acc(train_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOXQpc99Ttj"
   },
   "source": [
    "## Evaluate Trained Model\n",
    "\n",
    "Once we have the model trained, we need to see how it will perform on data that it was not trained on, that is, test data. \n",
    "\n",
    "We do this with Keras' evaluation function and the test dataset we retrieved earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yEEjOV69uay"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max training accuracy: 0.95568335   test accuracy: 0.8884\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "print('max training accuracy:', max(train_hist.history['accuracy']), '  test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAB01AmFAYw6"
   },
   "source": [
    "## Training Results\n",
    "\n",
    "The model has **about 96% accuracy** on the training data.  And **only 88% accuracy** on testing data on which it was not trained. This is a classic sign that the model overfits the training data.  \n",
    "\n",
    "We need to determine how we can reduce this overfitting and get **good accuracy on both training and test data!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6olDUcpiVkxe"
   },
   "source": [
    "# Monitoring and Improving our Trained Model’s Performance \n",
    "\n",
    "In this section, we want to make changes to improve the performance of our trained model.  By improved performance we mean having the model not overfit the training data and perform poorly on the testing data.\n",
    "\n",
    "There are serveral common techniques to fix this problem.  These include:\n",
    "\n",
    "1.   Reducing Model Complexity - removing neurons or layers\n",
    "2.   Dropout - Randomly removing the contributions from some neurons\n",
    "3.   Early Stopping - Terminating training as early as possible\n",
    "\n",
    "We will try these below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxM0Nan9ZjmK"
   },
   "source": [
    "## Monitoring performance with TensorBoard\n",
    "\n",
    "We want to ensure we are getting good data to evaluate the performance of our changes.  Fortunately the TensorFlow family include the tool **TensorBoard**.  \n",
    "\n",
    "TensorBoard provides us various ways to monitor the performance of our models including:\n",
    "\n",
    "*    Visualizing metrics such as loss and accuracy\n",
    "*    Comparision of training and evaluation metrics\n",
    "*    Visualizing the model graph (ops and layers)\n",
    "*    Viewing histograms of weights, biases, or other tensors as they change over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdydWnoYe7gy"
   },
   "source": [
    "TensorBoard can be run inside a Colab notebook, or if you are running your code directly in Python you can invoke TensorBoard from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3d1xtI7LjJyf"
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6dqMFnCbkC_"
   },
   "outputs": [],
   "source": [
    "# works only in colab - Load the tensorboard extension\n",
    "#% reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logs']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "glob(\"*log*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaXgSi2yi550"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ #only works on unix\n",
    "#recursive and don't prompt\n",
    "\n",
    "!rmdir /S /q \"logs\" \n",
    "#dels directory itself so remake it\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08RQKiAXi_Yz"
   },
   "outputs": [],
   "source": [
    "# Start with a fresh model\n",
    "model = tf.keras.models.Sequential()      # Create a new sequential model\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))    # keras processing layer - no neurons\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', name='dense-128-relu'))   # 128 neurons connected to pixels\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='dense-10-softmax')) # determine probability of each of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig5AjqzkjEwM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.4970 - accuracy: 0.8247 - val_loss: 0.4181 - val_accuracy: 0.8519\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.3779 - accuracy: 0.8638 - val_loss: 0.4058 - val_accuracy: 0.8497\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3404 - accuracy: 0.8766 - val_loss: 0.3894 - val_accuracy: 0.8582\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.3149 - accuracy: 0.8836 - val_loss: 0.3622 - val_accuracy: 0.8682\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2961 - accuracy: 0.8914 - val_loss: 0.3546 - val_accuracy: 0.8748\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2838 - accuracy: 0.8952 - val_loss: 0.3337 - val_accuracy: 0.8760\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2691 - accuracy: 0.8996 - val_loss: 0.3376 - val_accuracy: 0.8821\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2592 - accuracy: 0.9038 - val_loss: 0.3345 - val_accuracy: 0.8821\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2489 - accuracy: 0.9064 - val_loss: 0.3419 - val_accuracy: 0.8755\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2408 - accuracy: 0.9099 - val_loss: 0.3869 - val_accuracy: 0.8701\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2314 - accuracy: 0.9137 - val_loss: 0.3346 - val_accuracy: 0.8824\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2238 - accuracy: 0.9156 - val_loss: 0.3696 - val_accuracy: 0.8786\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2194 - accuracy: 0.9178 - val_loss: 0.3371 - val_accuracy: 0.8855\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2100 - accuracy: 0.9207 - val_loss: 0.3389 - val_accuracy: 0.8897\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2057 - accuracy: 0.9217 - val_loss: 0.3381 - val_accuracy: 0.8866\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1988 - accuracy: 0.9259 - val_loss: 0.3547 - val_accuracy: 0.8855\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1936 - accuracy: 0.9276 - val_loss: 0.3399 - val_accuracy: 0.8896\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1889 - accuracy: 0.9295 - val_loss: 0.3561 - val_accuracy: 0.8869\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1862 - accuracy: 0.9300 - val_loss: 0.3583 - val_accuracy: 0.8892\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1801 - accuracy: 0.9327 - val_loss: 0.3630 - val_accuracy: 0.8881\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1752 - accuracy: 0.9348 - val_loss: 0.3638 - val_accuracy: 0.8868\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1706 - accuracy: 0.9358 - val_loss: 0.3893 - val_accuracy: 0.8860\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1668 - accuracy: 0.9370 - val_loss: 0.3614 - val_accuracy: 0.8865\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1631 - accuracy: 0.9384 - val_loss: 0.3825 - val_accuracy: 0.8852\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1581 - accuracy: 0.9407 - val_loss: 0.3874 - val_accuracy: 0.8833\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1541 - accuracy: 0.9409 - val_loss: 0.4021 - val_accuracy: 0.8809\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1520 - accuracy: 0.9428 - val_loss: 0.3815 - val_accuracy: 0.8861\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1478 - accuracy: 0.9445 - val_loss: 0.4165 - val_accuracy: 0.8789\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.1467 - accuracy: 0.9436 - val_loss: 0.3954 - val_accuracy: 0.8918\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1417 - accuracy: 0.9460 - val_loss: 0.3868 - val_accuracy: 0.8902\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1394 - accuracy: 0.9470 - val_loss: 0.3965 - val_accuracy: 0.8887\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1353 - accuracy: 0.9492 - val_loss: 0.4136 - val_accuracy: 0.8889\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1336 - accuracy: 0.9500 - val_loss: 0.4115 - val_accuracy: 0.8864\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.1319 - accuracy: 0.9496 - val_loss: 0.4301 - val_accuracy: 0.8858\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1288 - accuracy: 0.9517 - val_loss: 0.4137 - val_accuracy: 0.8904\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1255 - accuracy: 0.9529 - val_loss: 0.4233 - val_accuracy: 0.8903\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1255 - accuracy: 0.9527 - val_loss: 0.4399 - val_accuracy: 0.8856\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1215 - accuracy: 0.9541 - val_loss: 0.4722 - val_accuracy: 0.8834\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1207 - accuracy: 0.9546 - val_loss: 0.4226 - val_accuracy: 0.8930\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1163 - accuracy: 0.9565 - val_loss: 0.4542 - val_accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#adjusted for windows\n",
    "log_dir='logs\\\\fit\\\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "#only works on colab?\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# We add to the fit method the validation/test data.  This will cause the training model \n",
    "# to evaluate itself on the validation/test data on each epoch.  This provides per \n",
    "# epoch data points TensorBoard can plot so we can see the trend.\n",
    "train_hist = model.fit(train_images, train_labels, epochs=40,\n",
    "                       validation_data=(test_images, test_labels), \n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cli: tensorboard --logdir logs\\fit\n",
    "url http://localhost:6006/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wx9ATGksjWyV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#may have 'delay'\n",
    "#!kill 1234   # sometime TensorBoard does not show all data.  If it shows reusing previous instance use kill command listed\n",
    "\n",
    "# %tensorboard --logdir logs/fit colab version\n",
    "#can't run like this b/c it never returns until ctrl+C (ctrl+break) pressed\n",
    "#!tensorboard --logdir logs\\fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v05icTF1mDfB"
   },
   "source": [
    "## Fixing Fashion MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6zDDPjrFm17"
   },
   "source": [
    "### Reducing Model Complexity\n",
    "\n",
    "In a large model we can consider reducing the number of hidden layers.  But our model only has one hidden layer and we need it.  So the only thing to do is reduce the number of neurons in the hidden layer.  Everthing else is the same \n",
    "as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1UvcFL9Qmps"
   },
   "outputs": [],
   "source": [
    "# Load the tensorboard extension\n",
    "#% reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNwSQKJyQmp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n",
      "Access is denied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\validation\\events.out.tfevents.1606265545.REDONE.7040.241775.v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logs\\fit\\202011~1\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2 - Access is denied.\n",
      "logs\\fit\\202011~1\\VALIDA~1\\events.out.tfevents.1606265545.REDONE.7040.241775.v2 - Access is denied.\n",
      "A subdirectory or file logs already exists.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ \n",
    "!del /S /Q \"logs\"\n",
    "!rmdir /S /q \"logs\" \n",
    "#dels directory itself so remake it\n",
    "!mkdir logs\n",
    "#permission errors when trying to clean up the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCUSydw2QmqJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()      # Create a new sequential model\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))    # keras processing layer - no neurons\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', name='dense-64-relu'))   # 64 neurons connected to pixels\n",
    "#model.add(tf.keras.layers.Dense(128, activation='relu', name='dense-128-relu'))   # 128 neurons connected to pixels\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='dense-10-softmax')) # determine probability of each of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8yt3SeaQmqN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.5193 - accuracy: 0.8210 - val_loss: 0.4677 - val_accuracy: 0.8375\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.3905 - accuracy: 0.8619 - val_loss: 0.4120 - val_accuracy: 0.8543\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.3546 - accuracy: 0.8729 - val_loss: 0.3959 - val_accuracy: 0.8596\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.3298 - accuracy: 0.8806 - val_loss: 0.3868 - val_accuracy: 0.8649\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.3143 - accuracy: 0.8852 - val_loss: 0.3671 - val_accuracy: 0.8698\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.3004 - accuracy: 0.8913 - val_loss: 0.3640 - val_accuracy: 0.8706\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2891 - accuracy: 0.8943 - val_loss: 0.3781 - val_accuracy: 0.8616\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2773 - accuracy: 0.8985 - val_loss: 0.3514 - val_accuracy: 0.8800\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2713 - accuracy: 0.9002 - val_loss: 0.3600 - val_accuracy: 0.8718\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2623 - accuracy: 0.9033 - val_loss: 0.3481 - val_accuracy: 0.8806\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2546 - accuracy: 0.9058 - val_loss: 0.3556 - val_accuracy: 0.8746\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2451 - accuracy: 0.9086 - val_loss: 0.3508 - val_accuracy: 0.8792\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2412 - accuracy: 0.9110 - val_loss: 0.3536 - val_accuracy: 0.8801\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2355 - accuracy: 0.9125 - val_loss: 0.3336 - val_accuracy: 0.8839\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2306 - accuracy: 0.9135 - val_loss: 0.3583 - val_accuracy: 0.8755\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2260 - accuracy: 0.9160 - val_loss: 0.3484 - val_accuracy: 0.8831\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2196 - accuracy: 0.9175 - val_loss: 0.3570 - val_accuracy: 0.8798\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2152 - accuracy: 0.9209 - val_loss: 0.3454 - val_accuracy: 0.8839\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2129 - accuracy: 0.9211 - val_loss: 0.3426 - val_accuracy: 0.8840\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2084 - accuracy: 0.9219 - val_loss: 0.3621 - val_accuracy: 0.8799\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2047 - accuracy: 0.9233 - val_loss: 0.3621 - val_accuracy: 0.8811\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2003 - accuracy: 0.9256 - val_loss: 0.3641 - val_accuracy: 0.8825\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1969 - accuracy: 0.9262 - val_loss: 0.3542 - val_accuracy: 0.8857\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1936 - accuracy: 0.9266 - val_loss: 0.3857 - val_accuracy: 0.8756\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1901 - accuracy: 0.9291 - val_loss: 0.3565 - val_accuracy: 0.8841\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1870 - accuracy: 0.9298 - val_loss: 0.3661 - val_accuracy: 0.8833\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1858 - accuracy: 0.9308 - val_loss: 0.3792 - val_accuracy: 0.8790\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1811 - accuracy: 0.9320 - val_loss: 0.3672 - val_accuracy: 0.8859\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.1789 - accuracy: 0.9337 - val_loss: 0.3930 - val_accuracy: 0.8808\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1778 - accuracy: 0.9326 - val_loss: 0.3917 - val_accuracy: 0.8799\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1742 - accuracy: 0.9341 - val_loss: 0.3743 - val_accuracy: 0.8838\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1715 - accuracy: 0.9351 - val_loss: 0.3800 - val_accuracy: 0.8847\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1698 - accuracy: 0.9361 - val_loss: 0.3873 - val_accuracy: 0.8828\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1672 - accuracy: 0.9372 - val_loss: 0.4057 - val_accuracy: 0.8808\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1658 - accuracy: 0.9376 - val_loss: 0.3910 - val_accuracy: 0.8847\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1637 - accuracy: 0.9385 - val_loss: 0.3825 - val_accuracy: 0.8850\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.1604 - accuracy: 0.9388 - val_loss: 0.4076 - val_accuracy: 0.8820\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1573 - accuracy: 0.9406 - val_loss: 0.4005 - val_accuracy: 0.8845\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1566 - accuracy: 0.9403 - val_loss: 0.4112 - val_accuracy: 0.8843\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1548 - accuracy: 0.9410 - val_loss: 0.4316 - val_accuracy: 0.8797\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "log_dir= 'logs\\\\fit2\\\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# We add to the fit method the validation/test data.  This will cause the training model \n",
    "# to evaluate itself on the validation/test data on each epoch.  This provides per \n",
    "# epoch data points TensorBoard can plot so we can see the trend.\n",
    "train_hist = model.fit(train_images, train_labels, epochs=40,\n",
    "                       validation_data=(test_images, test_labels), \n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ke-rDI6UmGaX"
   },
   "source": [
    "#### Show the results with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hox4ABkzmFWm"
   },
   "outputs": [],
   "source": [
    "#!kill 1234   # sometime TensorBoard does not show all data.  If it shows reusing previous instance use kill command listed\n",
    "#%tensorboard --logdir logs/fit\n",
    "#tensorboard --logdir logs\\fit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBwxV4HjFOwT"
   },
   "source": [
    "### Randomly dropout some neurons\n",
    "\n",
    "To randomly shut down the contribution from some neurons, we add a Keras dropout layer.  This layer randomly sets the outputs from the previous layer to 0.  How many outputs are set to 0 is defined by the parameter we pass.  To set 50% of \n",
    "the outputs to 0 pass 0.5.  For 20% pass 0.2.\n",
    "\n",
    "Also, the decision on which connections get dropped is a random selection that \n",
    "changes from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSMjk8zBpc2A"
   },
   "outputs": [],
   "source": [
    "# Load the tensorboard extension\n",
    "#% reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GcFNsSBpc2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n",
      "Access is denied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\validation\\events.out.tfevents.1606265545.REDONE.7040.241775.v2\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit2\\20201124-202828\\train\\events.out.tfevents.1606267708.REDONE.7040.793814.v2\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit2\\20201124-202828\\train\\events.out.tfevents.1606267708.REDONE.profile-empty\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit2\\20201124-202828\\train\\plugins\\profile\\2020-11-24_20-28-28\\local.trace\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit2\\20201124-202828\\validation\\events.out.tfevents.1606267711.REDONE.7040.808437.v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logs\\fit\\202011~1\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2 - Access is denied.\n",
      "logs\\fit\\202011~1\\VALIDA~1\\events.out.tfevents.1606265545.REDONE.7040.241775.v2 - Access is denied.\n",
      "A subdirectory or file logs already exists.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ \n",
    "!del /S /Q \"logs\"\n",
    "!rmdir /S /q \"logs\" \n",
    "#dels directory itself so remake it\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9SazRfplKGA"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()      # Create a new sequential model\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))    # keras processing layer - no neurons\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', name='dense-128-relu'))   # 128 neurons connected to pixels\n",
    "####ADDED DROPOUT LAYER####\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # dropout 20%\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='dense-10-softmax')) # determine probability of each of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM9BuhuBlWo0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jM5sr1YGlcIb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.5342 - accuracy: 0.8091 - val_loss: 0.4319 - val_accuracy: 0.8425\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4011 - accuracy: 0.8553 - val_loss: 0.3941 - val_accuracy: 0.8560\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.3662 - accuracy: 0.8665 - val_loss: 0.4079 - val_accuracy: 0.8498\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3441 - accuracy: 0.8736 - val_loss: 0.3785 - val_accuracy: 0.8636\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3292 - accuracy: 0.8785 - val_loss: 0.3522 - val_accuracy: 0.8684\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3146 - accuracy: 0.8841 - val_loss: 0.3518 - val_accuracy: 0.8734\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3062 - accuracy: 0.8850 - val_loss: 0.3606 - val_accuracy: 0.8709\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.2960 - accuracy: 0.8907 - val_loss: 0.3498 - val_accuracy: 0.8788\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2896 - accuracy: 0.8913 - val_loss: 0.3458 - val_accuracy: 0.8711\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2814 - accuracy: 0.8947 - val_loss: 0.3381 - val_accuracy: 0.8823\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2757 - accuracy: 0.8967 - val_loss: 0.3305 - val_accuracy: 0.8840\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2711 - accuracy: 0.8980 - val_loss: 0.3397 - val_accuracy: 0.8815\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.2654 - accuracy: 0.9004 - val_loss: 0.3312 - val_accuracy: 0.8855\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2579 - accuracy: 0.9036 - val_loss: 0.3328 - val_accuracy: 0.8861\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2553 - accuracy: 0.9031 - val_loss: 0.3343 - val_accuracy: 0.8878\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.2512 - accuracy: 0.9061 - val_loss: 0.3407 - val_accuracy: 0.8873\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2446 - accuracy: 0.9087 - val_loss: 0.3467 - val_accuracy: 0.8808\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2425 - accuracy: 0.9083 - val_loss: 0.3328 - val_accuracy: 0.8852\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2381 - accuracy: 0.9096 - val_loss: 0.3288 - val_accuracy: 0.8874\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2324 - accuracy: 0.9116 - val_loss: 0.3486 - val_accuracy: 0.8840\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2292 - accuracy: 0.9133 - val_loss: 0.3371 - val_accuracy: 0.8904\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2270 - accuracy: 0.9136 - val_loss: 0.3344 - val_accuracy: 0.8897\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2253 - accuracy: 0.9144 - val_loss: 0.3432 - val_accuracy: 0.8890\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2203 - accuracy: 0.9161 - val_loss: 0.3454 - val_accuracy: 0.8878\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2189 - accuracy: 0.9164 - val_loss: 0.3319 - val_accuracy: 0.8920\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.2166 - accuracy: 0.9179 - val_loss: 0.3506 - val_accuracy: 0.8862\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.2144 - accuracy: 0.9189 - val_loss: 0.3476 - val_accuracy: 0.8868\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2090 - accuracy: 0.9202 - val_loss: 0.3601 - val_accuracy: 0.8882\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.2053 - accuracy: 0.9216 - val_loss: 0.3647 - val_accuracy: 0.8850\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2068 - accuracy: 0.9213 - val_loss: 0.3632 - val_accuracy: 0.8881\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2039 - accuracy: 0.9220 - val_loss: 0.3477 - val_accuracy: 0.8907\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1985 - accuracy: 0.9242 - val_loss: 0.3676 - val_accuracy: 0.8847\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1994 - accuracy: 0.9235 - val_loss: 0.3472 - val_accuracy: 0.8916\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.1959 - accuracy: 0.9243 - val_loss: 0.3503 - val_accuracy: 0.8895\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.1936 - accuracy: 0.9271 - val_loss: 0.3664 - val_accuracy: 0.8821\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1929 - accuracy: 0.9267 - val_loss: 0.3612 - val_accuracy: 0.8869\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1889 - accuracy: 0.9277 - val_loss: 0.3564 - val_accuracy: 0.8884\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.1874 - accuracy: 0.9286 - val_loss: 0.3664 - val_accuracy: 0.8918\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.1872 - accuracy: 0.9282 - val_loss: 0.3780 - val_accuracy: 0.8903\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.1843 - accuracy: 0.9290 - val_loss: 0.3794 - val_accuracy: 0.8897\n",
      "Wall time: 2min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1da2befd488>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_dir='logs\\\\fit3\\\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=train_images, \n",
    "          y=train_labels, \n",
    "          epochs=40, \n",
    "          validation_data=(test_images, test_labels), \n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlUy6ZlRlOZ8"
   },
   "outputs": [],
   "source": [
    "#!kill 1234   # sometime TensorBoard does not show all data.  If it shows reusing previous instance use kill command listed\n",
    "#%tensorboard --logdir logs/fit\n",
    "#tensorboard --logdir logs\\fit3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wrym729ZWre"
   },
   "source": [
    "### Implementing Early Stopping\n",
    "\n",
    "Early stopping involves having the training stop when the model has been trained enough.  \n",
    "\n",
    "Often training is slow in terms of time, and expensive in terms of compute time on big clusters or a cloud service.  So it is useful to be able to stop training as soon as reasonable.  And reasonable is usually when the metric like lose quits decreasing.  So the question is, how can we know when the loss quits decreasing?  \n",
    "\n",
    "We could run a bunch of trials and use a tool like TensorBoard to plot curves.  But it is tedious to write a model with 20 epochs, run it, see loss was decreasing, then repeat that with 40 epochs, etc.  Worse, if we change the model or it's parameters, like adjusting the dropout percentage, the number of epochs required to see when the loss stops decreasing might be different.\n",
    "\n",
    "So what we need is an automated way to detect when training has quit improving and terminate training.  And fortunately Keras has an early stopping feature that does just this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lKTkBVvN2dI"
   },
   "source": [
    "We start with the same model, optimizer, loss, and log clearing code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0FSUxUT3Cuf"
   },
   "outputs": [],
   "source": [
    "# Load the tensorboard extension\n",
    "#% reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGm6LMEL2--j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2\n",
      "D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit\\20201124-195222\\validation\\events.out.tfevents.1606265545.REDONE.7040.241775.v2\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit3\\20201124-203524\\train\\events.out.tfevents.1606268124.REDONE.7040.1360413.v2\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit3\\20201124-203524\\train\\events.out.tfevents.1606268124.REDONE.profile-empty\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit3\\20201124-203524\\train\\plugins\\profile\\2020-11-24_20-35-24\\local.trace\n",
      "Deleted file - D:\\Users\\Rob\\Documents\\python\\tf37_tensorflow_ps\\logs\\fit3\\20201124-203524\\validation\\events.out.tfevents.1606268128.REDONE.7040.1375075.v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n",
      "Access is denied.\n",
      "logs\\fit\\202011~1\\train\\events.out.tfevents.1606265542.REDONE.7040.227152.v2 - Access is denied.\n",
      "logs\\fit\\202011~1\\VALIDA~1\\events.out.tfevents.1606265545.REDONE.7040.241775.v2 - Access is denied.\n",
      "A subdirectory or file logs already exists.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ \n",
    "!del /S /Q \"logs\"\n",
    "!rmdir /S /q \"logs\" \n",
    "#dels directory itself so remake it\n",
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62luPqB8NqaI"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()      # Create a new sequential model\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))    # keras processing layer - no neurons\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', name='dense-128-relu'))   # 128 neurons connected to pixels\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='dense-10-softmax')) # determine probability of each of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROXcHFavNqaS"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6TY_BVDOou_"
   },
   "source": [
    "We create a new EarlyStopping callback.\n",
    "\n",
    "And we set the **monitor** parameter to the parameter to monitor, which is **validation loss**. And the  **patience** parameter to the maximum number of epochs without improvement allowed.  And if the parameter does not improve within the patience number of epochs, the training will be terminated.\n",
    "\n",
    "This new callback gets added to the list of callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1o3tiyB_NyhV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.4934 - accuracy: 0.8278 - val_loss: 0.4346 - val_accuracy: 0.8459\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.3766 - accuracy: 0.8628 - val_loss: 0.3858 - val_accuracy: 0.8612\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3371 - accuracy: 0.8768 - val_loss: 0.3809 - val_accuracy: 0.8628\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.3161 - accuracy: 0.8848 - val_loss: 0.3557 - val_accuracy: 0.8752\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2962 - accuracy: 0.8908 - val_loss: 0.3424 - val_accuracy: 0.8794\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2820 - accuracy: 0.8956 - val_loss: 0.3440 - val_accuracy: 0.8734\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2683 - accuracy: 0.8990 - val_loss: 0.3285 - val_accuracy: 0.8849\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2569 - accuracy: 0.9046 - val_loss: 0.3202 - val_accuracy: 0.8878\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.2494 - accuracy: 0.9074 - val_loss: 0.3206 - val_accuracy: 0.8888\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2374 - accuracy: 0.9109 - val_loss: 0.3447 - val_accuracy: 0.8823\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2318 - accuracy: 0.9131 - val_loss: 0.3349 - val_accuracy: 0.8856\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.2226 - accuracy: 0.9169 - val_loss: 0.3412 - val_accuracy: 0.8808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1da2b6c7908>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir='logs\\\\fit4\\\\' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#### ADDED EARLY STOPPING CALLBACK ####\n",
    "#patience requires 4 observations of increasing loss before stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "model.fit(x=train_images, \n",
    "          y=train_labels, \n",
    "          epochs=40, \n",
    "          validation_data=(test_images, test_labels), \n",
    "          callbacks=[tensorboard_callback, early_stopping_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUIOxIxcQSRH"
   },
   "outputs": [],
   "source": [
    "#!kill 1234   # sometime TensorBoard does not show all data.  If it shows reusing previous instance use kill command listed\n",
    "#%tensorboard --logdir logs/fit\n",
    "#tensorboard --logdir logs\\fit4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwGPItyphqXT"
   },
   "source": [
    "## Save Your Model\n",
    "\n",
    "Now that we have our model working well.  We can save it for reuse.\n",
    "\n",
    "By saving the model's structure and the associated trained weights we preserve our work.  Those weights are well over 101,000 values for which we had to work hard to get to the proper numbers!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0w5Rq8SsgWE6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rob.DESKTOP-HBG5EOT\\.conda\\envs\\tf37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mysavedmodelv1\\assets\n"
     ]
    }
   ],
   "source": [
    "# We use the Python tempfile library to create files in a generated folder.  \n",
    "# If you want to used a defined path, replace this code with your own \n",
    "# path definitions. \n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# get the tempfile location for this Colab session\n",
    "#MODEL_DIR = tempfile.gettempdir()\n",
    "#version = 1   # NOTE: Adjust if you don't want to replace a version of your model\n",
    "#export_path = os.path.join(MODEL_DIR, str(version))  # the final path includes the version\n",
    "#print('Saving model to : {}\\n'.format(export_path))\n",
    "\n",
    "# if path already exists delete everything at the location\n",
    "#if os.path.isdir(export_path):\n",
    "#  print('\\nPreviously saved model found, deleting it\\n')\n",
    "#  !rm -r {export_path}\n",
    "\n",
    "# Save the model  \n",
    "tf.saved_model.save(model, 'mysavedmodelv1')\n",
    "\n",
    "# Print save complete message\n",
    "#print('Model saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YbP9MjFLkRJ"
   },
   "source": [
    "\n",
    "\n",
    "# Deploying our Trained Model\n",
    "\n",
    "Once we are satisified with the model's performance we can deploy it so other programs can use it.  This is a complex subject since the deployment strategies depend on how our model will be used.\n",
    "\n",
    "In this script we will show a very basic way of deploying the model.  We will do just enough so we can make predictions with our trained model from Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjXJXg2dZKlZ"
   },
   "source": [
    "## TensorFlow ModelServer\n",
    "\n",
    "We are going to use the Tensorflow ModelServer to serve our model.  This install may only work in Colab.  \n",
    "\n",
    "*The recommendation for general installation and usage of the server is to use a docker container.  This is documented in the repo, https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/setup.md*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTWb0-KjnHSP"
   },
   "source": [
    "## Add TensorFlow Serving as installable\n",
    "\n",
    "Before we can install the TensorFlow ModelServer we need to make it known to the APT installer so the installer know where to fetch the bits.\n",
    "\n",
    "We do this by adding a key in to the APT database refering to the tensorflow-serving files location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yadLd4SqnwMT"
   },
   "outputs": [],
   "source": [
    "# add the key\n",
    "!echo 'deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal' | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "# update the database with the new key\n",
    "!apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1ZVp_VOU7Wu"
   },
   "source": [
    "### Install TensorFlow ModelServer\n",
    "\n",
    "We use apt-get to fetch and install TensorFlow ModelServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygwa9AgRloYy"
   },
   "outputs": [],
   "source": [
    "!apt-get install tensorflow-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5NrYdQeVm52"
   },
   "source": [
    "### Start TensorFlow ModelServer\n",
    "\n",
    "This is where we start TensorFlow ModelServer and load our model.  After it loads we can start making inference requests using REST.  There are some important parameters:\n",
    "\n",
    "* `rest_api_port`: The port that you'll use for REST requests.\n",
    "* `model_name`: You'll use this in the URL of the REST requests.  It can be anything.\n",
    "* `model_base_path`: This is the path to the directory where you've saved your model.\n",
    "\n",
    "We need to define these as Python variables and as shell environment variables.  This will make this information available in Python code and in \n",
    "the shell where we will start the TensorFlow ModelServer process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUgp3vUdU5GS"
   },
   "outputs": [],
   "source": [
    "# Define the environment variable where our saved model resides\n",
    "os.environ['MODEL_DIR'] = MODEL_DIR\n",
    "# Define the Python constant and environment variable to point to the port number used to access our model\n",
    "REST_PORT = '8501'\n",
    "os.environ['REST_PORT'] = REST_PORT\n",
    "# Define the Python constant and environment variable to point to the our model name.\n",
    "#  This is an abitrary name.\n",
    "MODEL_NAME = 'fashion_mnist'\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJDhHNJVnaLN"
   },
   "outputs": [],
   "source": [
    "# Run the shell command to launch tensorflow_model_server.  Output status and error\n",
    "#  messages to the file server.log\n",
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=\"${REST_PORT}\" \\\n",
    "  --model_name=\"${MODEL_NAME}\" \\\n",
    "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxbeiOCUUs2z"
   },
   "outputs": [],
   "source": [
    "# Display the tail (last few lines) of the server log which will show any errors\n",
    "!tail server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwg1JKaGXWAg"
   },
   "source": [
    "### Function to display image with user defined title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Luqm_Jyff9iR"
   },
   "outputs": [],
   "source": [
    "def show_image(index, title, show_colorbar=False):\n",
    "  plt.figure()\n",
    "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
    "  plt.imshow(test_images[index].reshape(28,28), cmap='gray')  # data is grayscale, but displays in color without cmap='gray'\n",
    "  if (show_colorbar):\n",
    "    plt.colorbar()\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-Cc6WAFf4eg"
   },
   "source": [
    "## Make a REST request to predict class of our example image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReGh9AElrE4_"
   },
   "outputs": [],
   "source": [
    "!pip install -q requests    # Install the requests library which makes HTTP requests to the TensorFlow server \n",
    "                            # using our trained our model \n",
    "import requests     # import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dJFAJ9LqXMk"
   },
   "outputs": [],
   "source": [
    "# Select a random image to classify\n",
    "import random\n",
    "image_index = random.randint(0,len(test_images)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pll06kNWsCBF"
   },
   "outputs": [],
   "source": [
    " # Import json library and create the json data structure to be passed in the request.\n",
    "import json\n",
    "\n",
    "# The data is assumed to be a list of images, a 4D tensor of the shape [*,28,28,1].  \n",
    "# Convert the single 3d to 4d\n",
    "check_images = np.reshape(test_images[image_index],(-1,28,28,1))\n",
    "\n",
    "# Construct json data passed to server.  \"instances\" will hold the image(s) we \n",
    "#   want the model to classify\n",
    "data = json.dumps({'signature_name': 'serving_default', 'instances': check_images.tolist()})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gelYGr-s2RXw"
   },
   "outputs": [],
   "source": [
    "# Create the request\n",
    "\n",
    "# HTTP request header\n",
    "headers = {'content-type': 'application/json'}\n",
    "\n",
    "# Build the url to the service using the constants we defined earlier, should be of the form:\n",
    "#      http://localhost:8501/v1/models/fashion_mnist:predict\n",
    "predict_service_url = 'http://localhost:' + REST_PORT + '/v1/models/' + MODEL_NAME + ':predict'\n",
    "\n",
    "# Make request to the service.  Pass the headers and data.  Wait for the server's reponse.\n",
    "json_response = requests.post(predict_service_url, data=data, headers=headers)\n",
    "\n",
    "# Parse the response.  For each images we pass we get a list of probabilities \n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "# predictions[0] is the 10 probabilites for our first and only image.\n",
    "# predictions[0] has values in the form [0.1, 0.05, ...., 0.6]\n",
    "\n",
    "# The numpy argmax function returns the index of the highest value.  This is \n",
    "# the highest probabilty class predicted by our model. \n",
    "predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "# Display image, the actual class and the predicted class\n",
    "show_image(image_index, 'Model predicted class: {} (class {}). \\n Actually class: {} (class {})'.format(\n",
    "  class_names[predicted_class], predicted_class, class_names[test_labels[image_index]], test_labels[image_index]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fashion_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
