# Python Machine Learning 
by Sebastian Raschka and Vahid Mirjalili 
2019 Packt

notes from the book 

- convention - underscore after a attribute indicates that it is not created at init time (p 39)
- normalization procedcure helps gradient descent learning to covnergae more quickly (p 50)
- standardization helps with gradient descent learning b/c optimizer has to go through fewer steps (p 51)
- SSE may be non-zero even though all examples are classified - i.e. the model may report some amount of error even with 100% classification (p 51)
- stochastic part of SGD == weights are updated incrementally with each training example (p 52)
- error surface is noisier than in gradient descent
- shuffle the training dataset for every epoch
- adaptive learning rate with SGD rather than fixed learning rate
- online learning == model is trained on the fly as new data arrives
- partial_fit - method which does not reinitialize the weights for online learning
- train_test_split automatically shuffles the data
- startification - training and test subsets containing the same proportions of class labels (supervised)
- plot_decision_regions (algorithm by book authors on p60
- np.ravel (flatten a iter of iters) (p 54)
- OvR = One vs Rest - 
- **all allgorithms require informitive and discrimative features to yield useful predictions**

## logistic regression: easy to implement and performs well on linearly separable classes
  - logit function == logarithm of odds
  - log is used to refer to ln (natural log) thorughout the book :question:
  - uppercase pi symobl is like summation but multiplies over the specified range
  - log reduces the potential for numerical underflow - which can happen if the likelihoods are small
  - sklearn.linear_model.logisticRegression
  - solver paramter allows for different optimization algorithms: newton-cg, lbfgs(default in sklearn 0.22), lib-linear (default in sklearn < 0.22), sag, saga
  - np.argmax to get the largest column in each row - use for the labels
  - for predict sklearn expects a 2d matrix so must convert single row for input using reshape(1,-1)
  - goal is to maximize the conditional likelihoods which may increase outlier impact
- bias variance trade-off
  - high bias == underfitting == error (distance of predictions from actuals) - model is too simple to capture the underlying patterns
  - high variance == overfitting == consistency - works well on train but not on test
    - too many parameters?
  - regularization penalizes extreme paramater weights (lowers bias)
    - L1 regularization - (y2-y1) + (x2-x1)
    - L2 regularization == shrinkage == weight decay - distance formula - sqrt((y2-y1)^2+(x2-x1)^2)
    - requires all features to have comparable scale
    - lambda is regularization parameter sometimes specified as its inverse 'C'

## SVM = support vector machine
  - maximize the margin - the distance the decision boundary and data points
  - support vectors are the data points that are closest to the hyperplane boundary (they drive its location)
  - small margins can indicate overfitting
  - quadratic programming - what is this :question:
  - slack variable \gamma lowercase gamma -used for soft-margin classification - handles cases when data is not perfectly linearly separable
    - set C to control misclassification penalty
  - less prone to outliers than logistic regression
  - liblinear/libsvm == optimized library for solving logistic/SVM problems - work well if all data fits in memory - otherwise use SGDClassifer implementations
- kernel SVM = for nonlinear problems
  - kernel trick - protect the data into a higher dimensional space where it will be linearly separable
  - kernel is a  similarity function between a pair of examples
  - uses a mapping function represented by \phi lowercase phi
  - RBF (radial bias function) == Gaussian kernel - a widely used kernel 
  - cut-off paramater \gamma lowercase gamma - for gaussian sphere - higher menas bumpier decision boundary to better capture training but may lead to overfitting

## Decision Trees - 
explainable - help with dim reduction - no need to regularize inputs
- split based on the largest information gain
- prune == set maximum depth of the tree
- binary trees - to reduce the decision search space
- information gain - reduction in entropy from transforming a dataset - compare entropy before/after the change
  - mutual information - statistical dependence between two variables - name for information gain applied to a variable selection
- impurity measures - usually pruning cutoffs are more helpful (gini is usually roughly equivalent to entropy (scaled?))
  - gini impurity - minimize the probability of missclassification - usually falls between entropy and classification error
  - entropy - maximize mutual information ()
  - classification error - useful for pruning only (not growing) - less sensitive 
  - visualize with `sklearn.tree.plot_tree(model_name)`
  - graphviz has prettier tree visualizations but has many dependencies: better layout, colors 
    - outfile=None to bypass disk and save data to a variable instead
  - PydDotPlus - similar to graphviz and can convert \*.dot files to an image
  - graphviz has useful visualizations for decision trees


## Random forest
large number of small trees, based on random samples from input data
- random sample of data (with replacement)
- random subset of features chosen (without replacement) for each tree
- less interpretable than decision trees but less dependent on hyperparamter tuning
- bias variance tradeoff is controlled by the sample size
- d = number of features to consider at  each split = good starting point is sqrt(m) where m is the number of features in the training dataset
- 

## K-neaest neighbors
lazy learning - memorizes the training data rather than learning a discriminant function - very susceptible to overfitting
- parametrics models = estimate parameters from training data, can discard training data e.g. peceptron, logistic regression, SVM
- non-parametric models = number of parameters grows with training data, training data retained e.g. decision tree, kernel SVM
  -  instance based learning - subset of non-parametric - memorize training data (lazy)
-  for each new datapoint assign the new class by finding the k nearest points and the majority class is chosen for the new point
-  immediately adapts as new data is collected
-  right choice for _k_ is crucial for bias variance trade-off
-  

## data preprocessing
- find missing - fill(fillna) or remove(dropna) or Impute
  - sklearn.impute.SimpleImputer - mean imputation - mean of entire column - 
    - transformer class (fit and transform methods): run fit on train X data features (no y targets) - run transform on both train and test features (and prod)
- numpy vs pandas - sklearn has some support for DataFrames
  - sklearn support for np arrays is more mature - use when possible
- categorical data
  - ordinal - inherent order to the labels (e.g. Small Medium Large) - assign numbers based on index or mapping dictionary
    - sklearn.preprocessing.LabelEncoder later can use inverese_transform method to get labels back
  - nominal - no inherent order (e.g. red, blue, green) - require encoding
    - create columns from the values and code with 0/1
    - 1 column is typically removed as it is inherent from other selections and can cause multi-collinearity of the features
      - multi-collinearity causes matrices to be computationally difficult to invert
    - one-hot encoding - new dummy feature for each unique value in the nominal column 
      - sklearn.preprocessing.One-HotEncoder or sklearn.compose.ColumnTranformer to process mutliple columns at once
      - or pandas get_dummies method - drop_first = True to drop one of the categories
- splitting data - sklearn.model_selection.train_test_split()
  - stratify (see above)
  - common ratios train:test - 60/40 or 70/30 for small datasets - for large datasets it is common to have 90:10 or 99:1
  - common to re-train on whole dataset prior to deployment
- **feature scaling**
  - normalization - rescale features to range 0-1 (special case of min-max scaling) - sklearn.preprocesing.MinMaxScaler
  - standardization - alter the values to have unit variance and 0 mean - (subtract mean and divide by stddev) |x| might be larger than 1 - sklearn.preprocessing.StandardScaler
  - Robust scaling - scaled data according the 1st and 3rd quartiles (i.e. IQR) - sklearn.preprocessing.RobustScaler - recommended for small datasets with many outliers
- **Feature selection**
  - impose a penalty for models with large numbers of features
  - logistic reg with L1 regularization inherently causes weights to go to zero for less signficiant features - minimize model cost function and complexity penalty together
    - lbfgs does not support L1-regularized
    - get feature weights from lr.coef_
  - logistic reg with L2 regularization also causes reduction in weights but not as sparse
    - C is the inverse of the regularization parameter \lambda lambda
  - sequential feature selection
    - sequential backward selection - not implemented in sklearn but reasonably easy to code from scratch
      - Greedy algorithms - locally optimal selections at each stage
      - exhaustive search algorithms - check all possible combinations - more computation - more accurate
    - recursive backward elimination - 
    - tree based methods - feature_importances_ attribute after fitting RandomForestClassifier
  - if 2 features are highly correlated one may be ranked highly while the other is not fully captured
  - sklearn SelectFromModel selects features based on a user threshold (e.g. given a RandomForest and a threshold it will return features)

## Dimensionality reduction
- PCA - principal component analysis - unsupervised method - orthogonal vectors of maximum variance for a desired feature count
  - highly sensitive to data scaling
  - eigenvalues and eigenvectors based on covariance matrix of principal comopnents
    - np.linalg.eig (eigh avoids complex results that may come up with eig)
    - eigenvectors are typically scaled to unit length
  - total vs explained variance - bar chart of variance explained for each component and a cumsum total plotted as a line
  - signs for eigenvector matrix may be flipped depending on the LAPACK implementation on current system (does not effect the model) - mutliply by -1 to fix
  - sklearn.decompisition.PCA
    - setting n_components_ = None will return all components in sorted order instead of doing the dimensionality reduction
- LDA - Linear Discriminant analysis - supervised - find feature subspace the optimizes class seperability
  - assumes normally distributed data and that classes have identical covariance matrices
  - class labels taken into account in the form of mean vectors -computed for each class used for within-class and between-class scatter matrices
  - computing scatter matrix is the same as computing covariance matrix  (covariance matrix is normalized version)
  - instead of performing eigen decompoisition on the covariance matrix the eigenvalue problem is solved directly
- KPCA - kernel PCA
  -  use kernel function to map data into higher dimensional space that is linearly seperable - computationally expensive
  - results are projected onto the feature space directly without a transformation matrix
  - need to center the kernel matrix to guarantee that the feature space is centered at zero
  - most comon kernels
    - polynomial kernel
    - readial basis function (RBF) == Gaussian (see above)
  - not implemented in sklearn :question:
    - book solution uses scipy.spatial.distance.pdist and squareform; sciopy.exp; and scipy.linalg.eigh
  - must experiment to find the right value of gamma \gamma
  - creating datasets for testing
    - sklearn.datasets.make_moons
    - sklearn.datasets.make_circles
    - hyperbolic tangent == sigmoid kernel
  - from sklearn.dcomposition import KernelPCA (pg 133)\
  
## Model evaluation - fine tuning models and evalutating performance (pg 134)
- Pipeline class 'fit a model including an abitrary number of transformation steps ... and make predictions' (pg 134)
  - make_pipeline - supplied with arbitrary list of transformers (support fit/transmform methods) followed by an estimator that (implements fit/predict) (pg 136)
  - calling fit on the pipeline calls all the transformers followed by the fit method for estimator
  - calling predict on pipeline performs the same transformations but final step is predict method of estimator (pg 136)
  - no limit to # of intermediate steps
- using same test dataset repeatedly essentially makes it part of the model and leads to overfitting
  - solutions - goal obtain a lessbiased estimate of ability to generalize to new data
    - hold-out cross=validation - separate data into 3 parts train/validate/test - performance on validation data is used for model selection (pg 137)
      - con: perfomrance estimate may be ssensitive to how data is partitioned - estimate will vary for different data examples
    - k-fold cross-validation (p 137)
      - randomly split the training data into k-folds without replacement - k-1 folds used for training - last fold for validation
      - "average performance of the models based on the different, independent test folds"  (pg 138)
      - "use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values"  (pg 138)
      - good staring value for k is 10 (p 139)
      - use large k for small datasets = training in each iteration = lowers bias (but increases runtime)
      - large datsets use a smaller k (e.g. 5) 
      - stratified k-fold - to ensure each fold is representative of the class proportion (sklearn.modelSelection)
        - can yield beter bias and variance estimates
    -  leave one out cross validation (LOOCV) - set k to number of training samples (one training example per fold - recommended for small datasets (p 139)
    -  k-fold cross-validation scorer - sklearn.model_selection import cross_val_score - simplifies model evaluation (p 140)
      - use n_jobs param to use multiple CPUs
    - other methods: .643 bootstrap (p 141)
- use validation dataset for model selection

## learning curves and validation curves (p 141)
![image](https://user-images.githubusercontent.com/51385580/148666088-f6d48866-6383-41a0-90a1-baeea5fb9423.png)
  - common fixes
    - underfitting
      - increase parameters to the model
      - decrease degree of regularization (e.g. in SVM or logistic reg classifiers)
    - overfitting
      - collect more data
      - reduce model complexity
      - increase regularization
      - decrease features with feature selection
    - learning curve - uses stratified k-fold cross-validation to calculate the cross-validation accuracy of a classifier -  - `sklearn.model_selection import learning_curve`
      - cv param sets the number of folds (p 143)
      - ![image](https://user-images.githubusercontent.com/51385580/148666103-880070be-3d70-4695-9b8d-9e22264f1c7c.png)
      - pass a model or pipeline to the function to use for the validation
      - access parameters within the pipeline by referencing the name returend by model.get_params (need to check this) e.g. `logisticregression__C'
      - plot accuracy vs # training examples
    - validation curve  - uses stratified k-fold cross-validation by default to estimate the performance - `sklearn.model_selection import validation_curve` - (p 144) 
      -  plot of accuracy vs regularization parameter (C)
      -  ![image](https://user-images.githubusercontent.com/51385580/148666113-16e33148-d35d-4bbd-9ab0-f92a9d13f6c1.png)

## hyper-parameter tuning
- Grid Search - helps to improve performance by finding the optimal combination of hyperparamaters (p 145)
  - from sklearn.model_selection import GridSearchCV
    - pass a param_grid: list of dicts with param names and potential values to search (or single value to use)
    - "best_score_ attribute provides the accuracy score achieved (pg 146)
    -  best_params_ attribute provides the params used to attain the best score
    -  best_estimator_ among models which one had the best accuracy (using independent test datset)
    -  uses burte-force exhaustive search
    -  refit param - automatically refit whole training set to the best_estimator with best_params
-  Randomized search - usuall performs almost as well but is much more cost and time effective (p 146)
  -  RandomizedSearchCV
-  nested cross-validation - select among different machine learning algorithms ( p 147)
  -  outer k-fold cross-validation loop to split the data into training and test folds 
  -  inner loop is used to select the model using k-fold cross-validation on the training fold 
  -  5x2 for large datasets it is usueful to set CV=5 for outer loop and 2 for inner loop
  -  ![image](https://user-images.githubusercontent.com/51385580/148666513-e7558e5a-ceb5-47e4-a2bc-08d74dc1e7eb.png)
  -  cross_val_score function (where does this come from?)
-  other metrics for model perfomance - all in sklearn.metrics -  precision, recall, and the F1 score (p 148)
  - calculated from confusion matrix - sklearn.metrics import confusion_matrix (p148)
    -  visualize with matplotlibs mathshow
    -  predicted across top actuals vertical on the side row major labels are: TP, FN, FP, TN
    - "error can be understood as the sum of all false predictions divided by the number of total predictions, and the accuracy is calculated as the sum of correct predictions divided by the total number of predictions,"  (p 149) (FP+FN) / (FP+FN+TP+TN)
    - accuracy = (TP+TN)/(FP+FN+TP+TN) = 1-error
    - recall = TPR = TP/P = TP/(FN+TP) (p 149) minimizes the chance of missing something (FP are ok) (sklearn.metrics.recall_score)
    - useful for imbalanced class problems
      - FPR (false positive rate)= FP/N = FP/(FP+TN)
      - TPR (true positive rate) =  (same thing as recall) 
    - precision - emphasizes correctness i.e. minimizes the change of false positives = TP/(TP+FP (sklearn.metrics.precision_score)
    - F1 score - combines precision and recall  = 2* (PRExREC)/(PRE+REC) (sklearn.metrics.f1_score)
    - use one of these in GridSearchCV by passing to the `scoring` param
      - construct custom with skelarn.metrics.make_scorer (e.g. scorer = make_scorer(f1_score)
  - ROC (receiver operating characteristic) compares FPR and TPR to determine model performance - (p 150)
    - chart diagonal represents random guess performance
    - AUC (area under curve) is a numeric value for ROC curve to compare performance
      - interpolated teh average ROC curve from the multiple fuolds using scipy interp function (p 152)
      - obtain directly using `sklearn.metrics.roc_auc_score`
      - precision-recall curves - different probability thresholds
- multi-class classification scoring
  - "scikit-learn also implements macro and micro averaging methods to extend those scoring metrics to multiclass problems via one-vs.-all (OvA)"  (pg 152)
  - macro-averaging(default) - average of scores for different systems - sum of all precision metrics divided by the number of systems - useful if there might be a class imbalance
  - micro-averaging  - sum of TP for all systems divided by the sum all TP + sum of all FP for all systems - useful if you want to weight each instance/prediction equally (instance weight more than class weight)
  - specified to am make_socrer using the 'average' param
  - class imbalance - can get high accuracy just by pridcition the majority class - so accuracy based models are not adding value (i.e. don't use accuracy)
  - "assign a larger penalty to wrong predictions on the minority class." using class_weight='balanced' parameter  (pg 154)
  - ressample minority class - draw new samoles from training dataset with replacement
    - "sklearn.utils import resample"  (pg 154)
  - downsample the majority class (resample function can do this also)
  - Synthetic Minority Over-sampling Technique (SMOTE) generate sythetic data (not covered) (p 155)

## Ensemble models (p 156)
- majority voting - function for this defined on p 159 - sklearn v > 0.17 have it sklearn.ensemble.VotingClassifier
  - technically 'majority' refers to two classes; plurality voting is the correct terminology for 3+ classes (most still call it majority)
  - mode function used to determine most common class among participant models'
  - proof for inherent improvement (reduction in error) with ensemble performance (p 157)
    - binomical coefficient 9 above 2 in parens latex is 9 \choose 2
  - can use class probabilities from each model for majority voting instead of just the final class estimate - but requires classifiers to be well-calibrated (p 159)
    - "weighted majority vote based on class probabilities, we can again make use of NumPy, using np.average and np.argmax:"  (pg 159)
    - "np.argmax(np.bincount([0, 0, 1],...           weights=[0.2, 0.2, 0.6]))"; *weighted majority vote with argmax*  (pg 159)
    - "decision tree probabilities are calculated from a frequency vector"  (pg 162)
    - "k-nearest neighbors are aggregated to return the normalized class label frequencies in"  (pg 163)
- ensemble_error function - defined on (p 157)
- stacking - ensemble of classifiers in first layer feed their predictions to another level (typically Logistic regression) to produce a final prediction
- bagging - "draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating.";   (pg 168)
  - "from sklearn.ensemble import BaggingClassifier"  (pg 170)
  - "each sample used to fit a classifier, C_j, which is most typically an unpruned decision tree:"  (pg 169)
    - "random forests are a special case of bagging where we also use random feature subsets when fitting the individual decision trees"  (pg 169)
  - predictions combined using majority voting
  - "bagging can improve the accuracy of unstable models and decrease the degree of overfitting"  (pg 169)
  - ineffective at reducing model bias - so ensemble of mdodels should be clasifiers with low bias(p 172)
- boosting - use weaklearners to make improvements on poorly classified isntances - "focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples"  (pg 172)
  - "boosting algorithm uses random subsets of training examples drawn from the training dataset **without replacement**;"  (pg 172)
  - some sources indicate that ... "boosting can lead to a decrease in bias as well as variance compared to bagging models. In practice, however, boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data"  (pg 173)
  - AdaBoost - uses the complete training dataset (p 173)
    - "training examples are reweighted in each iteration" 
    - "assign a larger weight to the two previously misclassified examples (circles). Furthermore, we lower the weight of the correctly classified examples"  (pg 173)
    - "initialize the weights uniformly"; *to. sum to 1*  (pg 174)
    - after updates... "normalize the weights so that they sum up to 1"; *normalize to sum to 1 again after updates*  (pg 174)
    - sklearn.ensemble import AdaBosstClassifier
    - "need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance"  (pg 176)
  - gradient boosting
    - xgboost is based on original algorithm - computationally efficient
    - sklearn has GradientBoostClassifier and a faster vesrion in >0-.21 called HistGradientBoostingClassifier
  - adaptive boosting - weights are updated in a differnt fashoion (p 177)
- implementation of MajorityVoteClassifier  (pg 177)

## sentiment analysis (p 178)
also called 'opinion mining
- bag of words - based on word counts in each document from CountVectorizer
  - sklearn.feature_extraction.text import CountVectorizer
    - vacabulary_ attribute has all the words and their indices (usually assigned alphabetically)
    - also called term frequencies
  - n-grams - grab words together to capture context
    - e.g. n-grams of size 3-4 give good perfomance in SPAM filters
    - CountVectorizer has ngram_range parameter to handle
- tf-idf - term frequency inverse document frequency 
  - zero in denominator of calc so that words that are not in any document get a non-zero value (avoid division by zero?)
  - skelarn.feature_extraction.text import TfidfTransformer (takes CountVectorizer as input)
    - different computation than textbook - (adds 1 to numerator also) due to smooth_idf=True
    - class normalizes tfidfs directly using L2-normalization
  - TfidfVectorizer combbines CountVectorizer with TfidfTransformer (p 189)
- cleaning
  - strip unwanted characters e.g. punctuation
  - using regex to parse HTML is generally not advised
  - tokenize usnign str.split or NLTK
    - porter tokenizer
  - stemming
    - lemmatization - more computationally difficult - tries to stem to real dictionary word
    - porter stemmer - oldest/simplest 
    - lancaster stemmer - notorious for being more aggressive than porter
    - snowball stemmer (aka Porter2 or English) faster than porter 
    - "stemming and lemmatization have little impact on the performance of text classification"  (pg 187)- 
- Logistic regression of tfidf
  - recommended to use n_jobs to speed up grid search (specifically with regard to tokenization)
- Naive Bayes (not covered)
  - good for small datasets

- "**out-of-core learning** which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a dataset.Text"  (pg 191)
  - requires models with a partical_fit function (e.g. SGDClassifier)
  - right a function to load batches of data from files on disk (e.g. stream_docs function p 191)
  - use HashingVectorizer to avoid storing all words in memory - uses Hashing trick via 32 bit MurmurHash3 function (Austin Appleby)
    - choose a large number for  n_features param to avoid collisions
-word2vec - unsupervised - uses neural network - put words that have similar meanings into similar clusters
  - allows performaing math with words e.g. king - man + woman = queen

## topic modeling
---
## out of place notes and errata
*** NOTE that mlxtend has a plot_decision_regions function already defined that works like the one in this book ***
- "get_params method to get a basic idea of how we can access the individual parameters inside a GridSearchCV"  (pg 166) **move this up**
- "bad practice to use the test dataset more than once for model evaluation"  (pg 168) **move this up**
- python package pyprind is useful for putting progress bars on various parts of model training (p 179)
---

- "partial_fit function of SGDClassifier in scikit-learn to stream the documents"  (pg 191)
- "out-of-core learning, which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a dataset"  (pg 191)
- "HashingVectorizer is data-independent and makes use of the hashing trick via the 32-bit MurmurHash3 function by Austin Appleby"; ***used to circumvent need to have all words in memory***  (pg 192)
- "get_minibatch, that will take a document stream from the stream_docs"  (pg 192)
- "stream_docs, that reads in and returns one document at a time:>>> def stream_docs(path):"  (pg 192)
- "out-of-core learning is very memory efficient"  (pg 193)
- "by choosing a large number of features in HashingVectorizer, we reduce the chance of causing hash collisions"  (pg 193)
- "idea behind word2vec is to put words that have similar meanings into similar clusters"  (pg 194)
- "consider topic modeling as a clustering task"  (pg 194)
- "Latent Dirichlet Allocation (LDA). However, note that while Latent Dirichlet Allocation is often abbreviated as LDA, it is not to be confused with linear discriminant analysis,"  (pg 194)
- "modern alternative to the bag-of-words model is word2vec"  (pg 194)
- "Topic modeling describes the broad task of assigning topics to unlabeled text documents."  (pg 194)
- "LDA is quite involved and requires knowledge about Bayesian inference"  (pg 195)
- "probabilistic model that tries to find groups of words that appear frequently together"  (pg 195)
- "Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:A document-to-topic matrixA word-to-topic matrix"  (pg 195)
- "must define the number of topics beforehand"  (pg 195)
- "set the maximum document frequency of words to be considered to 10 percent (max_df=.1) to exclude words that occur too frequently"  (pg 196)
- "from sklearn.decomposition import LatentDirichletAllocation"  (pg 196)
- "setting learning_method='online' is analogous to online or mini-batch learning"  (pg 196)
- "uses the expectation-maximization (EM) algorithm to update its parameter estimates iteratively"  (pg 196)
- "values are ranked in increasing order. Thus, to print the top five words, we need to sort the topic array in reverse"  (pg 197)
- "out-of-core or incremental learning"  (pg 198)
- "One option for model persistence is Python's in-built pickle module"  (pg 199)
- "more efficient way to serialize NumPy arrays is to use the alternative joblib library"  (pg 200)
- "protocol=4 to choose the latest and most efficient pickle protocol"  (pg 200)
- "pickle module is not secured against malicious code"  (pg 201)
- "mapping dictionary should also be archived alongside the model"  (pg 202)
- "DB browser for SQLite app"  (pg 204)
- "first Flask web application"  (pg 204)
- "This first part of the app.py script"; ***imports; unpickle ; vectorizer setup; partial fit; logic***  (pg 210)
- "second part of the app.py"; ***forms; routing; rendering***  (pg 211)
- "transforms the predicted sentiment back into an integer class label that will be used to update the classifier via the train function"; ***based on correct or incorrect buttons***  (pg 212)
- "deploy our web application onto a public web server. For this tutorial, we will be using the PythonAnywhere"  (pg 214)
- "free beginner account"  (pg 214)
- "the clf object will be reset if the web server crashes or restarts"  (pg 215)
- "download the SQLite database from the PythonAnywhere server, update the clf object locally"  (pg 215)
- "update the classifier from the SQLite database every time we restart the web application."  (pg 216)
- "another subcategory of supervised learning: regression analysis"  (pg 218)
- "linear regression is to model the relationship between one or multiple features and a continuous target variable"  (pg 218)
- "this chapter will mainly focus on the univariate case"  (pg 219)
- "multiple linear regression hyperplanes in a three-dimensional scatterplot are already challenging to interpret"  (pg 219)
- "offsets or residuals—the errors of our prediction."  (pg 219)
- "plot the scatterplot matrix, we will use the scatterplotmatrix function from the MLxtend library"  (pg 221)
- "Exploratory data analysis (EDA) is an important and recommended first step prior to the training of a machine learning model"  (pg 221)
- "in contrast to common belief, training a linear regression model does not require that the explanatory or target variables are normally distributed. The normality assumption is only a requirement for certain statistics and hypothesis tests"  (pg 222)
- "We can interpret the correlation matrix as being a rescaled version of the covariance matrix. In fact, the correlation matrix is identical to a covariance"  (pg 222)
- "correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficient (often abbreviated as Pearson's r), which measures the linear dependence between pairs of features"  (pg 222)
- "Pearson's correlation coefficient can simply be calculated as the covariance between two features, x and y (numerator), divided by the product of their standard deviations"  (pg 222)
- "covariance between a pair of standardized features is, in fact, equal to their linear correlation coefficient"  (pg 222)
- "rescaled version of the covariance matrix."  (pg 222)
- "NumPy's corrcoef function on the five feature columns that we previously visualized in the scatterplot matrix, and we will use MLxtend's heatmap"; ***or seaborn***  (pg 223)
- "cost function in Adaline is the sum of squared errors (SSE), which is identical to the cost function that we use for OLS"  (pg 224)
- "OLS regression can be understood as Adaline without the unit step function so that we obtain continuous target values"  (pg 224)
- "helper function that will plot a scatterplot of the training examples and add the regression line:"  (pg 225)
- "good idea to plot the cost as a function of the number of epochs"  (pg 225)
- "workaround regarding y_std, using np.newaxis and flatten."  (pg 225)
- "standardize the variables for better convergence"  (pg 225)
- "y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()"  (pg 225)
- "technically don't have to update the weights of the intercept if we are working with standardized variables, since the y axis intercept is always 0 in those cases"  (pg 226)
- "many of scikit-learn's estimators for regression make use of the least squares implementation in SciPy (scipy.linalg.lstsq"  (pg 226)
- "works (better) with unstandardized variables"  (pg 226)
- "from sklearn.linear_model import LinearRegression"  (pg 226)
- "LinearRegression class implemented in MLxtend"  (pg 227)
- "￼"; ***normal equation,. may be too expensive too invert the matrix for large datasets***  (pg 227)
- "sklearn.linear_model import RANSACRegressor"  (pg 228)
- "stands for the median absolute deviation"  (pg 228)
- "regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers"  (pg 228)
- "ransac.inlier_mask_"  (pg 229)
- "appropriate value for the inlier threshold is problem-specific"  (pg 229)
- "Residual plots are a commonly used graphical tool for diagnosing regression models"  (pg 230)
- "example"; ***interpretation depends on the dataset***  (pg 231)
- "outliers, which are represented by the points with a large deviation from the centerline"  (pg 231)
- "mean squared error (MSE), which is simply the averaged value of the SSE cost that we minimized to fit the linear regression"  (pg 231)
- "MSE is unbounded"  (pg 231)
- "coefficient of determination (￼), which can be understood as a standardized version of the MSE"  (pg 231)
- "good regression model, we would expect the errors to be randomly distributed and the residuals to be randomly scattered around the centerline"  (pg 231)
- "L1 penalty for LASSO is defined as the sum of the absolute magnitudes of the model weights,"; ***can yield sparse models and be used for feature selection***  (pg 232)
- "hyperparameter ￼,"; ***increases regularization strength***  (pg 232)
- "Saturation of a model occurs if the number of training examples is equal to the number of features, which is a form of overparameterization"; ***becomes interpolation***  (pg 232)
- "sklearn.metrics import r2_score>"  (pg 232)
- "Ridge Regression is an L2 penalized model"  (pg 232)
- "don't regularize the intercept term, ￼."  (pg 232)
- "elastic net, which has an L1 penalty to generate sparsity and an L2 penalty such that it can be used for selecting more than n features if m > n"  (pg 232)
- "Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net"; ***regularized options***  (pg 232)
- "sklearn.linear_model import Ridge"  (pg 233)
- "sklearn.linear_model import Lasso"  (pg 233)
- "sklearn.linear_model import ElasticNet"  (pg 233)
- "l1_ratio to 1.0, the ElasticNet regressor would be equal to LASSO regression"  (pg 233)
- "PolynomialFeatures transformer class from scikit-learn to add a quadratic term (d = 2) to a simple regression problem"  (pg 233)
- "from sklearn.preprocessing import PolynomialFeatures>>> X = np.array(\[ 258.0, 270.0, 294.0, 320.0\],"  (pg 233)
- "y_quad_fit = pr.predict(quadratic.fit_transform(X_fit)"  (pg 234)
- "adding more and more polynomial features increases the complexity of a model and therefore increases the chance of overfitting"  (pg 235)
- "random forest regression"  (pg 236)
- "not require any transformation of the features if we are dealing with nonlinear data"  (pg 236)
- "the decision tree algorithm, we subdivide the input space into smaller regions that become more manageable"  (pg 236)
- "goal is to find the feature split that maximizes the information gain;"  (pg 237)
- "define the impurity measure of a node, t, as the MSE instead"  (pg 237)
- "MSE is often referred to as within-node variance, which is why the splitting criterion is also better known as variance reduction"  (pg 237)
- "from sklearn.tree import DecisionTreeRegressor"  (pg 237)
- "random forests are that they are less sensitive to outliers"  (pg 238)
- "use the MSE criterion to grow the individual decision trees"  (pg 238)
- "sklearn.ensemble import RandomForestRegressor"  (pg 238)
- "improve the model by transforming variables, tuning the hyperparameters of the learning algorithm, choosing simpler or more complex models, removing outliers, or including additional variables"  (pg 239)
- "patterns in the prediction errors, for example, by inspecting the residual plot, it means that the residual plots contain predictive information"  (pg 239)
- "SVMs can also be used in nonlinear regression tasks"  (pg 240)
- "k-means, which is widely used in academia as well as in industry."  (pg 241)
- "Prototype-based clustering means that each cluster is represented by a prototype, which is usually either the centroid (average) of similar points with continuous features, or the medoid"  (pg 241)
- "to specify the number of clusters, k, a priori"; ***drawback***  (pg 242)
- "elbow method and silhouette plots, which are useful techniques to evaluate the quality of a clustering"  (pg 242)
- "from sklearn.datasets import make_blobs"  (pg 242)
- "Based on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia:"  (pg 243)
- "from sklearn.cluster import KMeans"  (pg 243)
- "max_iter parameter, we specify the maximum number of iterations for each single run"  (pg 243)
- "One way to deal with convergence problems is to choose larger values for tol, which is a parameter that controls the tolerance with regard to the changes in the within-cluster SSE"  (pg 243)
- "n_init=10 to run the k-means clustering algorithms 10 times"  (pg 243)
- "problem with k-means is that one or more clusters can be empty"; ***workaround in current sklearn impl. farthest point from empty centroid reassigned as new centroid***  (pg 243)
- "make sure that the features are measured on the same scale and apply z-score standardization or min-max scaling"  (pg 244)
- "clusters do not overlap and are not hierarchical"  (pg 244)
- "place the initial centroids far away from each other via the k-means++ algorithm"  (pg 245)
- "Hard clustering describes a family of algorithms where each example in a dataset is assigned to exactly one cluster,"  (pg 245)
- "soft clustering (sometimes also called fuzzy clustering) assign an example to one or more clusters."  (pg 245)
- "the init parameter to 'k-means++'"; ***this is the default init for kmeans***  (pg 245)
- "typically m = 2), is the so-called fuzziness coefficient (or simply fuzzifier"  (pg 246)
- "fuzzy C-means (FCM) algorithm (also called soft k-means or fuzzy k-means"  (pg 246)
- "replace the hard cluster assignment with probabilities for each point belonging to each cluster"  (pg 246)
- "note that the membership indicator, ￼, is not a binary value as in k-means"  (pg 246)
- "elbow method, to estimate the optimal number of clusters, k, for a given task."  (pg 247)
- "quantify the quality of clustering, we need to use intrinsic metrics—such as the within-cluster SSE (distortion)"  (pg 247)
- "identify the value of k where the distortion begins to increase most rapidly"  (pg 247)
- "each iteration in FCM is more expensive than an iteration in k-means. On the other hand, FCM typically requires fewer iterations overall"; ***not implemented in sklearn***  (pg 247)
- "Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples in the clusters"  (pg 248)
- "cluster cohesion, ￼, as the average distance between an example"  (pg 248)
- "cluster separation, ￼, from the next closest cluster"  (pg 248)
- "silhouette coefficient is bounded in the range –1 to 1."  (pg 248)
- "from sklearn.metrics import silhouette_samples"  (pg 248)
- "silhouette coefficients are not even close to 0, which is, in this case, an indicator of a good clustering"  (pg 249)
- "two main approaches to hierarchical clustering are agglomerative and divisive hierarchical clustering"  (pg 250)
- "advantage of the hierarchical clustering algorithm is that it allows us to plot dendrograms (visualizations of a binary hierarchical clustering"  (pg 250)
- "typically do not have the luxury of visualizing datasets in two-dimensional scatterplots in real-world problems"  (pg 250)
- "single linkage, we compute the distances between the most similar members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest. The complete linkage approach is similar to single linkage but, instead of comparing the most similar members in each pair of clusters, we compare the most dissimilar members to perform the merge."  (pg 251)
- "agglomerative clustering using the complete linkage"  (pg 251)
- "Ward's linkage, the two clusters that lead to the minimum increase of the total within-cluster SSE are merged"  (pg 251)
- "average linkage, we merge the cluster pairs based on the minimum average distances between all group members"  (pg 251)
- "should not use the squareform distance matrix"  (pg 252)
- "linkage function from SciPy's cluster.hierarchy submodule, which returns a so-called linkage matrix"  (pg 252)
- "calculate the distance matrix as input for the hierarchical clustering algorithm, we will use the pdist function from SciPy's spatial.distance submodule"; ***agglomerative clustering. without sklearn***  (pg 252)
- "hierarchical clustering dendrograms are often used in combination with a heat map"; ***how does this heat map help?***  (pg 254)
- "density-based spatial clustering of applications with noise (DBSCAN)"  (pg 255)
- "from sklearn.cluster import AgglomerativeClustering"  (pg 255)
- "allows us to choose the number of clusters that we want to return. This is useful if we want to prune"  (pg 255)
- "doesn't necessarily assign each point to a cluster but is capable of removing noise points"  (pg 256)
- "does not assume that the clusters have a spherical shape"  (pg 256)
- "labeling the points as core, border, or noise"  (pg 256)
- "density-based clustering assigns cluster labels based on dense regions of points. In DBSCAN, the notion of density is defined as the number of points within a specified radius, ￼."  (pg 256)
- "from sklearn.cluster import DBSCAN"  (pg 257)
- "successful clustering does not only depend on the algorithm and its hyperparameters; rather, the choice of an appropriate distance metric and the use of domain knowledge"  (pg 258)
- "the eigenvectors of a similarity or distance matrix to derive the cluster relationships"  (pg 258)
- "graph-based clustering. Probably the most prominent members of the graph-based clustering family are the spectral clustering"  (pg 258)
- "common practice to apply dimensionality reduction techniques prior to performing clustering"  (pg 258)
- "two hyperparameters in DBSCAN (MinPts and ￼) that need to be optimized to yield good clustering results"  (pg 258)
- "disadvantages of DBSCAN"; ***works poorly if large number of features***  (pg 258)
- "particularly common to compress datasets down to two-dimensional subspaces, which allows us to visualize"  (pg 259)
- "deep neural network (DNN) architectures that are particularly well suited for image and text analyses"  (pg 260)
- "backpropagation algorithm to train NNs more efficiently"  (pg 260)
- "voice recognition"  (pg 261)
- "learning rate ￼"  (pg 262)
- "fully connected network is also called MLP."  (pg 263)
- "added noise can help to escape local cost minima,"  (pg 263)
- "SGD approximates the cost from a single training sample (online learning) or a small subset of training examples (mini-batch learning"  (pg 263)
- "single-layer network because of its single link between the input and output layers"  (pg 263)
- "threshold function"  (pg 263)
- "activation function ￼"  (pg 263)
- "more than one hidden layer, we also call it a deep artificial NN"  (pg 263)
- "can think of the number of layers and units in an NN as additional hyperparameters that we want to optimize for a given problem task"  (pg 263)
- "error gradients, which we will calculate later via backpropagation, will become increasingly small as more layers are added to a network. This vanishing gradient"  (pg 264)
- "forward propagation to calculate the network output"  (pg 265)
- "backpropagate the error"  (pg 265)
- "calculate the error that we want to minimize using a cost function"  (pg 265)
- "forward propagate the patterns of the training data"  (pg 265)
- "-hot vector representation allows us to tackle classification tasks with an arbitrary number of unique class labels present in the training"  (pg 265)
- "forward propagation to calculate the output of an MLP model"  (pg 265)
- "can think of the neurons in the MLP as logistic regression units that return values in the continuous range between 0 and 1."  (pg 266)
- "feedforward refers to the fact that each layer serves as the input to the next layer without loops"  (pg 266)
- "activation function, which has to be differentiable to learn the weights that connect the neurons"  (pg 266)
- "our first multilayer NN to classify handwritten digits"  (pg 267)
- "unroll the ￼ pixels into one-dimensional row vectors"  (pg 268)
- "commonly used trick for improving convergence in gradient-based optimization through input scaling is batch normalization"  (pg 269)
- "scaled the images on a pixel-by-pixel basis, which is different from the feature scaling approach"  (pg 269)
- "efficient and convenient method to save multidimensional arrays to disk is NumPy's savez function"  (pg 271)
- "savez_compressed"  (pg 271)
- "savez function creates zipped archives of our data, producing .npz files that contain files in the .npy format"  (pg 271)
- "MLP from scratch to classify the images in the MNIST dataset"  (pg 272)
- "scikit-learn's new fetch_openml"  (pg 272)
- "Shuffles training data every epoch        if True to prevent circles"  (pg 273)
- "relatively large gap between the training and the validation accuracy indicated that the model is likely overfitting"  (pg 277)
- "training the NN may take up to five minutes"  (pg 277)
- "eval_ attribute that collects the cost, training, and validation accuracy for each epoch"  (pg 278)
- "stop it early in certain circumstances and start over with different hyperparameter settings"  (pg 278)
- "change the learning rate during training"  (pg 279)
- "performance-enhancing tricks such as adaptive learning rates, more sophisticated SGD-based optimization algorithms, batch normalization, and dropout"  (pg 279)
- "Attaching loss functions to earlier layers in the networks"  (pg 279)
- "Another useful technique to tackle overfitting in NNs is dropout,"  (pg 279)
- "Adding skip-connections, which are the main contribution of residual NNs"  (pg 279)
- "backpropagation algorithm, which allows us to calculate those partial derivatives to minimize the cost function"  (pg 281)
- "we don't regularize the bias units)"  (pg 281)
- "backpropagation as a very computationally efficient approach to compute the partial derivatives of a complex cost function"  (pg 282)
- "computer algebra, a set of techniques has been developed to solve such problems very efficiently, which is also known as automatic differentiation"  (pg 282)
- "bumps in this high-dimensional cost surface (local minima) that we have to overcome"  (pg 282)
- "backpropagation is simply a special case of reverse-mode automatic differentiation"  (pg 283)
- "function derivatives, partial derivatives, gradients, and the Jacobian"  (pg 283)
- "￼ symbol means element-wise multiplication in this context"; ***circle containing a dot***  (pg 284)
- "Mini-batch learning has the advantage over online learning that we can make use of our vectorized implementations to improve computational efficiency"  (pg 285)
- "increasing the learning rate, we can more readily escape such local minima. On the other hand, we also increase the chance of overshooting the global optimum if the learning rate is too large"  (pg 285)
- "simplifying APIs such as Keras have been developed that make the construction of common deep learning models even more convenient"  (pg 286)
- "writing code to target GPUs is not as simple as executing Python code in our interpreter. There are special packages, such as CUDA and OpenCL"  (pg 288)
- "TensorFlow is a scalable and multiplatform programming interface for implementing and running machine learning algorithms, including convenience wrappers for deep learning"  (pg 288)
- "TensorFlow supports CUDA-enabled GPUs officially. Support for OpenCL-enabled devices is still experimental"  (pg 289)
- "scalar can be defined as a rank-0 tensor, a vector can be defined as a rank-1 tensor, a matrix can be defined as a rank-2 tensor, and matrices stacked in a third dimension can be defined as rank-3 tensors"  (pg 289)
- "TensorFlow library recently received a major overhaul with its 2.0 version"  (pg 289)
- "train your models using a GPU for free via Google Colab."  (pg 291)
- "create a tensor from a list or a NumPy array using the tf.convert_to_tensor"  (pg 291)
- "certain operations require that the input tensors have a certain number of dimensions (that is, rank)"  (pg 292)
- "tf.transpose(), tf.reshape(), and tf.squeeze()."  (pg 292)
- ",..."  (pg 292)
- "tf.random.uniform(shape=(5, 2),"  (pg 292)
- "tf.cast() function can be used to change the data type"  (pg 292)
- "tf.random.normal(shape=(5, 2),"  (pg 292)
- "tf.multiply(t1, t2).numpy()"; ***element wise product***  (pg 292)
- "tf.linalg.matmul(t1, t2, transpose_b=True)"; ***matrix matrix product ( cross product)***  (pg 293)
- "compute the mean, sum, and standard deviation along a certain axis (or axes), we can use tf.math.reduce_mean(), tf.math.reduce_sum(), and tf.math.reduce_std()."  (pg 293)
- "tf.norm() function is useful for computing the ￼ norm of a tensor"  (pg 293)
- "tf.split() function, which divides an input tensor into a list of equally-sized tensors."  (pg 293)
- "concatenate or stack them to create a single tensor. In this case, TensorFlow functions such as tf.stack() and tf.concat()"  (pg 294)
- "typical use cases, however, when the dataset is too large to fit into the computer memory, we will need to load the data from the main storage"  (pg 295)
- "common source of error could be that the element-wise correspondence between the original features"  (pg 296)
- ".batch() method has an optional argument, drop_remainder, which is useful for cases when the number of elements in the tensor is not divisible by the desired batch size."  (pg 296)
- "buffer_size, which determines how many elements in the dataset are grouped together before shuffling"  (pg 297)
- "tf.io to read the image file contents, and tf.image to decode the raw contents"  (pg 299)
- "common source of error is to call .batch() twice"  (pg 299)
- "tensorflow_datasets library provides a nice collection of freely available datasets"  (pg 301)
- "we want to pass this dataset to a supervised deep learning model during training, we have to reformat it as a tuple of (features, label"  (pg 303)
- "function called load() that combines the three steps for fetching a dataset in one"  (pg 304)
- "commonly used approach for building an NN in TensorFlow is through tf.keras.Sequential(),"  (pg 305)
- "TensorFlow 2.0, tf.keras has become the primary and recommended approach for implementing models"  (pg 305)
- "compile() and .fit() methods"  (pg 306)
- "tf.keras.Model. This gives us more control over the forward pass by"  (pg 306)
- "Building model layers and parameters after instantiation by calling the .build() method is called late variable creation"  (pg 307)
- "specify the dimensionality of the input (the number of features) to this model. We can do this by calling model.build()"  (pg 307)
- "Rather than manually computing the gradients, we will use the TensorFlow API tf.GradientTape"  (pg 308)
- "pass the NumPy arrays for x and y directly, without needing to create a dataset"  (pg 309)
- "convenient .fit() method that can be called on an instantiated model. To show how this works, let's create a new model and compile it by selecting the optimizer, loss function, and evaluation metrics"  (pg 309)
- "already defined layers through tf.keras.layers that can be readily used"  (pg 310)
- ".take() and .skip() to split the dataset to two partitions"  (pg 310)
- "tensorflow_datasets library provides a convenient tool that allows us to determine slices and splits via the DatasetBuilder"  (pg 310)
- "reshuffle_each_iteration=False"; ***avoids mixing train and test***  (pg 311)
- "three output neurons, since we have three class labels"  (pg 311)
- "tf.keras.layers.Dense), which is also known as a fully connected (FC) layer or linear layer,"  (pg 311)
- "build an infinitely repeating dataset, which will be passed to the fit() method for training"  (pg 312)
- "used the sigmoid activation function for the first layer and softmax activation for the last (output) layer"  (pg 312)
- "history = iris_model.fit(ds_train, epochs=num_epochs,...                          steps_per_epoch=steps_per_epoch,...                          verbose=0"  (pg 313)
- "results = iris_model.evaluate(ds_test.batch(50), verbose=0)"  (pg 313)
- "here (for evaluation), the size of the batch does not matter"  (pg 313)
- "returned variable history keeps the training loss and the training accuracy (since they were specified as metrics to iris_model.compile())"  (pg 313)
- "iris_model.save('iris-classifier.h5',overwrite=True, include_optimizer=True, save_format='h5"; ***save model for future use***  (pg 313)
- "have to batch the test dataset as well,"; ***even if test is small because bagging adds dimension(rank) that is expected by the model b/c train data had it***  (pg 313)
- "want to introduce nonlinearity in a typical artificial NN to be able to tackle complex problems"  (pg 314)
- "can use any function as an activation function in multilayer NNs as long as it is differentiable."  (pg 314)
- "sigmoid) activation function can be problematic if we have highly negative input"  (pg 314)
- "iris_model.to_json() method, which saves the model configuration in JSON format. Or if you want to save only the model weights, you can do that by calling iris_model.save_weights()"  (pg 314)
- "people often prefer a hyperbolic tangent as an activation function in hidden layers"; ***for negative input only?***  (pg 315)
- "generalization of the logistic function, the softmax"  (pg 316)
- "softmax function is a soft form of the argmax function; instead of giving a single class index, it provides the probability of each class"  (pg 316)
- "sum of the exponentially weighted linear functions:"  (pg 316)
- "One way to predict the class label from the output units obtained earlier is to use the maximum value:>>> y_class = np.argmax(Z, axis=0)>>> print(‘Predicted class label: %d’ % y_class)Predicted class label: 0"  (pg 316)
- "advantage of the hyperbolic tangent over the logistic function is that it has a broader output spectrum ranging in the open interval (–1, 1), which can improve the convergence of the back-propagation algorithm"  (pg 317)
- "hyperbolic tangent (commonly known as tanh), which can be interpreted as a rescaled version of the logistic function"  (pg 317)
- "vanishing gradient problem of tanh and logistic activations"  (pg 318)
- "derivative of activations with respect to the net input diminishes as z becomes large"  (pg 318)
- "from scipy.special import expit"; ***logistic function***  (pg 318)
- "Rectified linear unit (ReLU)"  (pg 318)
- "tf.keras.activations.tanh("  (pg 318)
- "np.tanh(z)"  (pg 318)
- "tf.keras.activations.sigmoid(z)"  (pg 318)
- "derivative of ReLU, with respect to its input, is always 1 for positive input values. Therefore, it solves the problem of vanishing gradients"  (pg 319)
- "￼You can find the list of all activation functions available in the Keras API at https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations."  (pg 319)
- "EM distance can be interpreted as the minimal amount of work needed to transform one distribution into the other"  (pg 442)
- "difference between recursion and dynamic programming is that dynamic programming stores the results of subproblems"  (pg 454)
- "dynamic programming is about recursive problem solving—"  (pg 454)
- "regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers"  (pg 228)
- "Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net"  (pg 232)

