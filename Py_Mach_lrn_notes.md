# Python Machin Learning by xxx and yyyy

notes from the book 

- convention - underscore after a attribute indicates that it is not created at init time (p 39)
- normalization procedcure helps gradient descent learning to covnergae more quickly (p 50)
- standardization helps with gradient descent learning b/c optimizer has to go through fewer steps (p 51)
- SSE may be non-zero even though all examples are classified - i.e. the model may report some amount of error even with 100% classification (p 51)
- stochastic part of SGD == weights are updated incrementally with each training example (p 52)
- error surface is noisier than in gradient descent
- shuffle the training dataset for every epoch
- adaptive learning rate with SGD rather than fixed learning rate
- online learning == model is trained on the fly as new data arrives
- partial_fit - method which does not reinitialize the weights for online learning
- train_test_split automatically shuffles the data
- startification - training and test subsets containing the same proportions of class labels (supervised)
- plot_decision_regions (algorithm by book authors on p60
- np.ravel (flatten a iter of iters) (p 54)
- OvR = One vs Rest - 
- **all allgorithms require informitive and discrimative features to yield useful predictions**

## logistic regression: easy to implement and performs well on linearly separable classes
  - logit function == logarithm of odds
  - log is used to refer to ln (natural log) thorughout the book :question:
  - uppercase pi symobl is like summation but multiplies over the specified range
  - log reduces the potential for numerical underflow - which can happen if the likelihoods are small
  - sklearn.linear_model.logisticRegression
  - solver paramter allows for different optimization algorithms: newton-cg, lbfgs(default in sklearn 0.22), lib-linear (default in sklearn < 0.22), sag, saga
  - np.argmax to get the largest column in each row - use for the labels
  - for predict sklearn expects a 2d matrix so must convert single row for input using reshape(1,-1)
  - goal is to maximize the conditional likelihoods which may increase outlier impact
- bias variance trade-off
  - high bias == underfitting == error (distance of predictions from actuals) - model is too simple to capture the underlying patterns
  - high variance == overfitting == consistency - works well on train but not on test
    - too many parameters?
  - regularization penalizes extreme paramater weights (lowers bias)
    - L1 regularization - (y2-y1) + (x2-x1)
    - L2 regularization == shrinkage == weight decay - distance formula - sqrt((y2-y1)^2+(x2-x1)^2)
    - requires all features to have comparable scale
    - lambda is regularization parameter sometimes specified as its inverse 'C'

## SVM = support vector machine
  - maximize the margin - the distance the decision boundary and data points
  - support vectors are the data points that are closest to the hyperplane boundary (they drive its location)
  - small margins can indicate overfitting
  - quadratic programming - what is this :question:
  - slack variable \gamma lowercase gamma -used for soft-margin classification - handles cases when data is not perfectly linearly separable
    - set C to control misclassification penalty
  - less prone to outliers than logistic regression
  - liblinear/libsvm == optimized library for solving logistic/SVM problems - work well if all data fits in memory - otherwise use SGDClassifer implementations
- kernel SVM = for nonlinear problems
  - kernel trick - protect the data into a higher dimensional space where it will be linearly separable
  - kernel is a  similarity function between a pair of examples
  - uses a mapping function represented by \phi lowercase phi
  - RBF (radial bias function) == Gaussian kernel - a widely used kernel 
  - cut-off paramater \gamma lowercase gamma - for gaussian sphere - higher menas bumpier decision boundary to better capture training but may lead to overfitting

## Decision Trees - 
explainable - help with dim reduction - no need to regularize inputs
- split based on the largest information gain
- prune == set maximum depth of the tree
- binary trees - to reduce the decision search space
- information gain - reduction in entropy from transforming a dataset - compare entropy before/after the change
  - mutual information - statistical dependence between two variables - name for information gain applied to a variable selection
- impurity measures - usually pruning cutoffs are more helpful (gini is usually roughly equivalent to entropy (scaled?))
  - gini impurity - minimize the probability of missclassification - usually falls between entropy and classification error
  - entropy - maximize mutual information ()
  - classification error - useful for pruning only (not growing) - less sensitive 
  - visualize with `sklearn.tree.plot_tree(model_name)`
  - graphviz has prettier tree visualizations but has many dependencies: better layout, colors 
    - outfile=None to bypass disk and save data to a variable instead
  - PydDotPlus - similar to graphviz and can convert \*.dot files to an image
  - graphviz has useful visualizations for decision trees


## Random forest
large number of small trees, based on random samples from input data
- random sample of data (with replacement)
- random subset of features chosen (without replacement) for each tree
- less interpretable than decision trees but less dependent on hyperparamter tuning
- bias variance tradeoff is controlled by the sample size
- d = number of features to consider at  each split = good starting point is sqrt(m) where m is the number of features in the training dataset
- 

## K-neaest neighbors
lazy learning - memorizes the training data rather than learning a discriminant function - very susceptible to overfitting
- parametrics models = estimate parameters from training data, can discard training data e.g. peceptron, logistic regression, SVM
- non-parametric models = number of parameters grows with training data, training data retained e.g. decision tree, kernel SVM
  -  instance based learning - subset of non-parametric - memorize training data (lazy)
-  for each new datapoint assign the new class by finding the k nearest points and the majority class is chosen for the new point
-  immediately adapts as new data is collected
-  right choice for _k_ is crucial for bias variance trade-off
-  

## data preprocessing
- find missing - fill(fillna) or remove(dropna) or Impute
  - sklearn.impute.SimpleImputer - mean imputation - mean of entire column - 
    - transformer class (fit and transform methods): run fit on train X data features (no y targets) - run transform on both train and test features (and prod)
- numpy vs pandas - sklearn has some support for DataFrames
  - sklearn support for np arrays is more mature - use when possible
- categorical data
  - ordinal - inherent order to the labels (e.g. Small Medium Large) - assign numbers based on index or mapping dictionary
    - sklearn.preprocessing.LabelEncoder later can use inverese_transform method to get labels back
  - nominal - no inherent order (e.g. red, blue, green) - require encoding
    - create columns from the values and code with 0/1
    - 1 column is typically removed as it is inherent from other selections and can cause multi-collinearity of the features
      - multi-collinearity causes matrices to be computationally difficult to invert
    - one-hot encoding - new dummy feature for each unique value in the nominal column 
      - sklearn.preprocessing.One-HotEncoder or sklearn.compose.ColumnTranformer to process mutliple columns at once
      - or pandas get_dummies method - drop_first = True to drop one of the categories
- splitting data - sklearn.model_selection.train_test_split()
  - stratify (see above)
  - common ratios train:test - 60/40 or 70/30 for small datasets - for large datasets it is common to have 90:10 or 99:1
  - common to re-train on whole dataset prior to deployment
- **feature scaling**
  - normalization - rescale features to range 0-1 (special case of min-max scaling) - sklearn.preprocesing.MinMaxScaler
  - standardization - alter the values to have unit variance and 0 mean - (subtract mean and divide by stddev) |x| might be larger than 1 - sklearn.preprocessing.StandardScaler
  - Robust scaling - scaled data according the 1st and 3rd quartiles (i.e. IQR) - sklearn.preprocessing.RobustScaler - recommended for small datasets with many outliers
- **Feature selection**
  - impose a penalty for models with large numbers of features
  - logistic reg with L1 regularization inherently causes weights to go to zero for less signficiant features - minimize model cost function and complexity penalty together
    - lbfgs does not support L1-regularized
    - get feature weights from lr.coef_
  - logistic reg with L2 regularization also causes reduction in weights but not as sparse
    - C is the inverse of the regularization parameter \lambda lambda
  - sequential feature selection
    - sequential backward selection - not implemented in sklearn but reasonably easy to code from scratch
      - Greedy algorithms - locally optimal selections at each stage
      - exhaustive search algorithms - check all possible combinations - more computation - more accurate
    - recursive backward elimination - 
    - tree based methods - feature_importances_ attribute after fitting RandomForestClassifier
  - if 2 features are highly correlated one may be ranked highly while the other is not fully captured
  - sklearn SelectFromModel selects features based on a user threshold (e.g. given a RandomForest and a threshold it will return features)

## Dimensionality reduction
- PCA - principal component analysis - unsupervised method - orthogonal vectors of maximum variance for a desired feature count
  - highly sensitive to data scaling
  - eigenvalues and eigenvectors based on covariance matrix of principal comopnents
    - np.linalg.eig (eigh avoids complex results that may come up with eig)
    - eigenvectors are typically scaled to unit length
  - total vs explained variance - bar chart of variance explained for each component and a cumsum total plotted as a line
  - signs for eigenvector matrix may be flipped depending on the LAPACK implementation on current system (does not effect the model) - mutliply by -1 to fix
  - sklearn.decompisition.PCA
    - setting n_components_ = None will return all components in sorted order instead of doing the dimensionality reduction
- LDA - Linear Discriminant analysis - supervised - find feature subspace the optimizes class seperability
  - assumes normally distributed data and that classes have identical covariance matrices
  - class labels taken into account in the form of mean vectors -computed for each class used for within-class and between-class scatter matrices
  - computing scatter matrix is the same as computing covariance matrix  (covariance matrix is normalized version)
  - instead of performing eigen decompoisition on the covariance matrix the eigenvalue problem is solved directly
- KPCA - kernel PCA
  -  use kernel function to map data into higher dimensional space that is linearly seperable - computationally expensive
  - results are projected onto the feature space directly without a transformation matrix
  - need to center the kernel matrix to guarantee that the feature space is centered at zero
  - most comon kernels
    - polynomial kernel
    - readial basis function (RBF) == Gaussian (see above)
  - not implemented in sklearn :question:
    - book solution uses scipy.spatial.distance.pdist and squareform; sciopy.exp; and scipy.linalg.eigh
  - must experiment to find the right value of gamma \gamma
  - creating datasets for testing
    - sklearn.datasets.make_moons
    - sklearn.datasets.make_circles
    - hyperbolic tangent == sigmoid kernel
  - from sklearn.dcomposition import KernelPCA (pg 133)\
  
## Model evaluation - fine tuning models and evalutating performance (pg 134)
- Pipeline class 'fit a model including an abitrary number of transformation steps ... and make predictions' (pg 134)
  - make_pipeline - supplied with arbitrary list of transformers (support fit/transmform methods) followed by an estimator that (implements fit/predict) (pg 136)
  - calling fit on the pipeline calls all the transformers followed by the fit method for estimator
  - calling predict on pipeline performs the same transformations but final step is predict method of estimator (pg 136)
  - no limit to # of intermediate steps
- using same test dataset repeatedly essentially makes it part of the model and leads to overfitting
  - solutions - goal obtain a lessbiased estimate of ability to generalize to new data
    - hold-out cross=validation - separate data into 3 parts train/validate/test - performance on validation data is used for model selection (pg 137)
      - con: perfomrance estimate may be ssensitive to how data is partitioned - estimate will vary for different data examples
    - k-fold cross-validation (p 137)
      - randomly split the training data into k-folds without replacement - k-1 folds used for training - last fold for validation
      - "average performance of the models based on the different, independent test folds"  (pg 138)
      - "use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values"  (pg 138)
      - good staring value for k is 10 (p 139)
      - use large k for small datasets = training in each iteration = lowers bias (but increases runtime)
      - large datsets use a smaller k (e.g. 5) 
      - stratified k-fold - to ensure each fold is representative of the class proportion (sklearn.modelSelection)
        - can yield beter bias and variance estimates
    -  leave one out cross validation (LOOCV) - set k to number of training samples (one training example per fold - recommended for small datasets (p 139)
    -  k-fold cross-validation scorer - sklearn.model_selection import cross_val_score - simplifies model evaluation (p 140)
      - use n_jobs param to use multiple CPUs
    - other methods: .643 bootstrap, 
- use validation dataset for model selection

## learning curves and validation curves (p 141)
  - common fixes
    - underfitting
      - increase parameters to the model
      - decrease degree of regularization (e.g. in SVM or logistic reg classifiers)
    - overfitting
     - collect more data
     - reduce model complexity
     - increase regularization
     - decrease features with feature selection

# notes exported from ebook app


- "collect more training examples to reduce the degree of overfitting"  (pg 141)
- "setting n_jobs=2, we could distribute the 10 rounds of cross-validation to two CPUs"  (pg 141)
- "alternative cross-validation techniques, such as the .632 Bootstrap cross-validation method"  (pg 141)
- "underfits the training data. Common ways to address this issue are to increase the number of parameters of the model, for example, by collecting or constructing additional features, or by decreasing the degree of regularization, for example, in support vector machine (SVM) or logistic regression classifiers"  (pg 142)
- "problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter, for example"  (pg 142)
- "decrease the number of features via feature selection"  (pg 142)
- "learning curve function from scikit-learn to evaluate the model"  (pg 142)
- "from sklearn.model_selection import learning_curve"  (pg 142)
- "default, the learning_curve function uses stratified k-fold cross-validation to calculate the cross-validation accuracy of a classifier, and we set k=10 via the cv parameter for 10-fold stratified cross-validation"  (pg 143)
- "from sklearn.model_selection import validation_curve"  (pg 144)
- "validation_curve function uses stratified k-fold cross-validation by default to estimate the performance"  (pg 144)
- "we wrote as 'logisticregression__C' to access the LogisticRegression object inside the scikit-learn pipeline"  (pg 144)
- "grid search, which can further help to improve the performance of a model by finding the optimal combination of hyperparameter values"  (pg 145)
- "brute-force exhaustive search"; *grid search*  (pg 145)
- "-selected model, which is available via the best_estimator_"; *for use with test data*  (pg 146)
- "RandomizedSearchCV class in scikit-"  (pg 146)
- "Randomized search usually performs about as well as grid search but is much more cost- and time-effective"  (pg 146)
- "GridSearchCV class has a refit parameter, which will refit the gs.best_estimator_ to the whole training set automatically"  (pg 146)
- "best_score_ attribute and looked at its parameters, which can be accessed via the best_params_"  (pg 146)
- "param_grid parameter of GridSearchCV to a list of dictionaries to specify the parameters that we'd want to tune. For the linear SVM"  (pg 146)
- "outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold"  (pg 147)
- ""; *inner loop CV=2*  (pg 147)
- "cross_val_score"; *outer loop CV=5*  (pg 147)
- "If we want to select among different machine learning algorithms, though, another recommended approach is nested cross-validation"  (pg 147)
- "nested cross-validation with only five outer and two inner folds, which can be useful for large datasets where computational performance is important; this particular type of nested cross-validation is also known as 5x2 cross-validation:￼"  (pg 147)
- "Matplotlib's matshow"  (pg 148)
- "other performance metrics that can be used to measure a model's relevance, such as precision, recall, and the F1 score."  (pg 148)
- "scikit-learn provides a convenient confusion_matrix"  (pg 148)
- "optimizing for recall helps with minimizing the chance of not detecting a malignant tumor."  (pg 149)
- "precision (PRE) and recall (REC) are related to those TP and TN rates, and in fact, REC is synonymous with TPR:"  (pg 149)
- "positive rate (TPR) and false positive rate (FPR) are performance metrics that are especially useful for imbalanced class problems"  (pg 149)
- "combination of PRE and REC is used, the so-called F1 score:￼"  (pg 149)
- "precision, on the other hand, we emphasize correctness"  (pg 149)
- "error can be understood as the sum of all false predictions divided by the number of total predictions, and the accuracy is calculated as the sum of correct predictions divided by the total number of predictions,"  (pg 149)
- "use a different scoring metric than accuracy in the GridSearchCV"  (pg 150)
- "construct our own scorer via the make_scorer function"  (pg 150)
- "Receiver operating characteristic (ROC) graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR"  (pg 150)
- "precision-recall curves for different probability thresholds"  (pg 150)
- "diagonal of a ROC graph can be interpreted as random guessing"  (pg 150)
- "Micro-averaging is useful if we want to weight each instance or prediction equally"; *instance weight more than class weight?*  (pg 152)
- "macro-average is used by default"  (pg 152)
- "roc_auc_score function from the sklearn.metrics"  (pg 152)
- "scikit-learn also implements macro and micro averaging methods to extend those scoring metrics to multiclass problems via one-vs.-all (OvA)"  (pg 152)
- "interp function that we imported from SciPy"  (pg 152)
- "-average is useful if we are dealing with class imbalances, that is, different numbers of instances for each label"  (pg 153)
- "when we fit classifiers on such datasets, it would make sense to focus on other metrics than accuracy"; *can get 90% accuracy just by predicting majority class; models below that accuracy are not adding anything*  (pg 153)
- "can specify the averaging method via the average"  (pg 153)
- "assign a larger penalty to wrong predictions on the minority class."  (pg 154)
- "sklearn.utils import resample"  (pg 154)
- "resample function that can help with the upsampling of the minority class by drawing new samples from the dataset with replacement."  (pg 154)
- "Synthetic Minority Over-sampling Technique (SMOTE"  (pg 155)
- "could downsample the majority class"  (pg 155)
- "ensemble can be built from different classification algorithms,"  (pg 156)
- "generalize the majority voting principle to multiclass settings, which is called plurality voting"  (pg 156)
- "error rate of the ensemble (0.034) is much lower than the error rate of each individual classifier (0.25)"  (pg 157)
- "￼ is the binomial coefficient n choose k."  (pg 157)
- "ensemble_error"; *they will probably reuse this later*  (pg 157)
- ""majority voting" will be used for simplicity, as is often the case in the literature."  (pg 158)
- "Using the predicted class probabilities instead of the class labels for majority voting can be useful if the classifiers in our ensemble are well calibrated"  (pg 159)
- "weighted majority vote based on class probabilities, we can again make use of NumPy, using np.average and np.argmax:"  (pg 159)
- "np.argmax(np.bincount([0, 0, 1],...           weights=[0.2, 0.2, 0.6]))"; *weighted majority vote with argmax*  (pg 159)
- "sklearn.ensemble.VotingClassifier in scikit-learn version 0.17 and newer"  (pg 162)
- "decision trees, the probabilities are calculated from a frequency vector"  (pg 162)
- "k-nearest neighbors are aggregated to return the normalized class label frequencies in"  (pg 163)
- "get_params method to get a basic idea of how we can access the individual parameters inside a GridSearchCV"  (pg 166)
- "bad practice to use the test dataset more than once for model evaluation"  (pg 168)
- "stacking algorithm can be understood as a two-level ensemble, where the first level consists of individual classifiers that feed their predictions to the second level, where another classifier (typically logistic regression) is fit to the level-one classifier predictions to make the final predictions"  (pg 168)
- "draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating."; *bagging*  (pg 168)
- "classifier, ￼, which is most typically an unpruned decision tree:"  (pg 169)
- "bagging can improve the accuracy of unstable models and decrease the degree of overfitting"  (pg 169)
- "random forests are a special case of bagging where we also use random feature subsets when fitting the individual decision trees"  (pg 169)
- "predictions are combined using majority voting"  (pg 169)
- "from sklearn.ensemble import BaggingClassifier"  (pg 170)
- "bagging is ineffective in reducing model bias"  (pg 172)
- "boosting is to focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples"  (pg 172)
- "boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement;"  (pg 172)
- "boosting, the ensemble consists of very simple base classifiers, also often referred to as weak learners"  (pg 172)
- "want to perform bagging on an ensemble of classifiers with low bias"  (pg 172)
- "AdaBoost uses the complete training dataset"  (pg 173)
- "training examples are reweighted in each iteration"  (pg 173)
- "assign a larger weight to the two previously misclassified examples (circles). Furthermore, we lower the weight of the correctly classified examples"  (pg 173)
- "tendency to overfit the training"  (pg 173)
- "boosting can lead to a decrease in bias as well as variance compared to bagging models. In practice, however, boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data"  (pg 173)
- "normalize the weights so that they sum up to 1"; *normalize to sum to 1 again after updates*  (pg 174)
- "initialize the weights uniformly"; *to. sum to 1*  (pg 174)
- "practice to select a model based on the repeated usage of the test dataset"  (pg 175)
- "introduced additional variance by our attempt to reduce the model bias"  (pg 175)
- "AdaBoostClassifier"  (pg 175)
- "need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance"  (pg 176)
- "adaptive and gradient boosting, differ mainly with regard to how the weights are updated and how the (weak) classifiers are combined."  (pg 177)
- "XGBoost, which is essentially a computationally efficient implementation of the original gradient boost algorithm"  (pg 177)
- "gradient boosting"  (pg 177)
- "implemented MajorityVoteClassifier"  (pg 177)
- "GradientBoostingClassifier implementation in scikit-learn, scikit-learn now also includes a substantially faster version of gradient boosting in version 0.21, HistGradientBoostingClassifier"  (pg 177)
- "sentiment analysis, sometimes also called opinion mining"  (pg 178)
- "stemming and lemmatization have little impact on the performance of text classification"  (pg 187)
- "lemmatization is computationally more difficult and expensive compared to stemming and"  (pg 187)
- "of-core learning, which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a dataset.Text"  (pg 191)
- "Latent Dirichlet Allocation (LDA). However, note that while Latent Dirichlet Allocation is often abbreviated as LDA, it is not to be confused with linear discriminant analysis,"  (pg 194)
- "regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers"  (pg 228)
- "Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net"  (pg 232)
- "￼You can find the list of all activation functions available in the Keras API at https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations."  (pg 319)
- "EM distance can be interpreted as the minimal amount of work needed to transform one distribution into the other"  (pg 442)
- "dynamic programming is about recursive problem solving—"  (pg 454)
- "difference between recursion and dynamic programming is that dynamic programming stores the results of subproblems"  (pg 454)
